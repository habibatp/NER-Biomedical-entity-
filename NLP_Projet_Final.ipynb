{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e47b82132ae45c5a7885549bd129481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa01a4354a1a40b1a3e43d71824f4f23",
              "IPY_MODEL_83d7aea1126043a1a633895d7dee6b33",
              "IPY_MODEL_3ddd05b58b354b48a78c5d3dff8eb5ed"
            ],
            "layout": "IPY_MODEL_c3008be2e9fd45dea255457984ca5c07"
          }
        },
        "aa01a4354a1a40b1a3e43d71824f4f23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b89d949cd6fa4a9e8c5f707d49d4d639",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2305a10f125947f6b4e2c5ca59125da2",
            "value": "config.json:‚Äá100%"
          }
        },
        "83d7aea1126043a1a633895d7dee6b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_138925f6cf354a2492760073fd4f3360",
            "max": 313,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_328a33d62d8840a388ce191626d751fa",
            "value": 313
          }
        },
        "3ddd05b58b354b48a78c5d3dff8eb5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b4ab68e00504436ac6648f7398a164f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_22a2387e2eb74876ae7ca82ad0acc3a4",
            "value": "‚Äá313/313‚Äá[00:00&lt;00:00,‚Äá12.0kB/s]"
          }
        },
        "c3008be2e9fd45dea255457984ca5c07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89d949cd6fa4a9e8c5f707d49d4d639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2305a10f125947f6b4e2c5ca59125da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "138925f6cf354a2492760073fd4f3360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328a33d62d8840a388ce191626d751fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b4ab68e00504436ac6648f7398a164f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a2387e2eb74876ae7ca82ad0acc3a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24a722f799014450ab2f6e029a0dd2ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56dac657eba44020aac66e44f23b9685",
              "IPY_MODEL_022b3f98e8624576a99b191c29e2c158",
              "IPY_MODEL_6384df2a6fe74f07950afb71f3638984"
            ],
            "layout": "IPY_MODEL_d8d3ea612f6f4773b6d4b1e024744c85"
          }
        },
        "56dac657eba44020aac66e44f23b9685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_541b25b3f4924e009e1701fb9aa9d38f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bc6c8426bd9a4be692574f88f6684335",
            "value": "vocab.txt:‚Äá100%"
          }
        },
        "022b3f98e8624576a99b191c29e2c158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_545e966959ed4a1a93ee6454e7c627d8",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6320a4fb998540119f7be776b4c2c6b1",
            "value": 213450
          }
        },
        "6384df2a6fe74f07950afb71f3638984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d18369616244492a74220e9917a000b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3fdaf3bbcc5341fca341cf4fefcce10c",
            "value": "‚Äá213k/213k‚Äá[00:00&lt;00:00,‚Äá2.91MB/s]"
          }
        },
        "d8d3ea612f6f4773b6d4b1e024744c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541b25b3f4924e009e1701fb9aa9d38f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc6c8426bd9a4be692574f88f6684335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "545e966959ed4a1a93ee6454e7c627d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6320a4fb998540119f7be776b4c2c6b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d18369616244492a74220e9917a000b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fdaf3bbcc5341fca341cf4fefcce10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install biopython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYNljYvJTxac",
        "outputId": "d81fb155-51bc-444d-f56e-55ccf12a0934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.0.2)\n",
            "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m179.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG6wWZ4xRzuE",
        "outputId": "4f110272-cb56-45c9-94ef-6e13677f8eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîé Recherche en cours : human diseases\n",
            "üîé Recherche en cours : disease symptoms\n",
            "üîé Recherche en cours : human gene\n",
            "üîé Recherche en cours : human protein\n",
            "‚úÖ Corpus m√©dical enregistr√© dans 'corpus_medical.csv'\n"
          ]
        }
      ],
      "source": [
        "from Bio import Entrez\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# ‚úÖ Identifiant requis pour utiliser l'API de la NCBI\n",
        "Entrez.email = \"amizmizhabiba6@gmail.com\"  # Remplace par ton vrai email\n",
        "\n",
        "# ‚úÖ Fonction pour interroger PubMed\n",
        "def get_pubmed_abstracts(query, max_results=80):\n",
        "    handle = Entrez.esearch(db=\"pubmed\", term=query, retmax=max_results)\n",
        "    record = Entrez.read(handle)\n",
        "    ids = record[\"IdList\"]\n",
        "\n",
        "    articles = []\n",
        "    for pubmed_id in ids:\n",
        "        try:\n",
        "            fetch_handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, rettype=\"abstract\", retmode=\"text\")\n",
        "            abstract = fetch_handle.read()\n",
        "            articles.append((pubmed_id, abstract))\n",
        "            time.sleep(0.5)  # Pour √©viter de surcharger l‚ÄôAPI\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur pour l'ID {pubmed_id} : {e}\")\n",
        "    return articles\n",
        "\n",
        "# ‚úÖ Requ√™tes associ√©es aux cat√©gories m√©dicales\n",
        "queries = {\n",
        "    \"Disease\": \"human diseases\",\n",
        "    \"Symptom\": \"disease symptoms\",\n",
        "    \"Gene\": \"human gene\",\n",
        "    \"Protein\": \"human protein\"\n",
        "}\n",
        "\n",
        "# ‚úÖ Extraction des donn√©es\n",
        "data = []\n",
        "\n",
        "for category, query in queries.items():\n",
        "    print(f\"üîé Recherche en cours : {query}\")\n",
        "    articles = get_pubmed_abstracts(query, max_results=80)\n",
        "    for pmid, abstract in articles:\n",
        "        data.append({\"PMID\": pmid, \"Category\": category, \"Abstract\": abstract.strip()})\n",
        "\n",
        "# ‚úÖ Enregistrement dans un fichier CSV\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"corpus_medical.csv\", index=False)\n",
        "print(\"‚úÖ Corpus m√©dical enregistr√© dans 'corpus_medical.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Compter directement depuis le DataFrame df\n",
        "count_by_category = df['Category'].value_counts()\n",
        "print(\"üìä Nombre de corpus par cat√©gorie :\")\n",
        "print(count_by_category)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOKrOzU9T-CG",
        "outputId": "884c509b-8eb0-4326-c5b9-25145ea1c400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Nombre de corpus par cat√©gorie :\n",
            "Category\n",
            "Disease    80\n",
            "Symptom    80\n",
            "Gene       80\n",
            "Protein    80\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqaFkTwwUvb7",
        "outputId": "43d2e996-6016-48d1-b48b-c9ede97d4699"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Stopwords anglais de base + adverbes courants ajout√©s manuellement\n",
        "stop_words = set([\n",
        "    \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\",\n",
        "    \"are\", \"aren't\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\",\n",
        "    \"between\", \"both\", \"but\", \"by\", \"could\", \"couldn't\", \"did\", \"didn't\", \"do\", \"does\",\n",
        "    \"doesn't\", \"doing\", \"don't\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\",\n",
        "    \"had\", \"hadn't\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\",\n",
        "    \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",\n",
        "    \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"isn't\", \"it\",\n",
        "    \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\",\n",
        "    \"no\", \"nor\", \"not\", \"of\", \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\",\n",
        "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\",\n",
        "    \"she's\", \"should\", \"shouldn't\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\",\n",
        "    \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\",\n",
        "    \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\",\n",
        "    \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\", \"we'll\", \"we're\",\n",
        "    \"we've\", \"were\", \"weren't\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\",\n",
        "    \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"won't\", \"would\",\n",
        "    \"wouldn't\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\",\n",
        "\n",
        "    # Adverbes courants ajout√©s\n",
        "    \"furthermore\", \"additionally\", \"however\", \"moreover\", \"nevertheless\",\n",
        "    \"thus\", \"therefore\", \"hence\", \"indeed\", \"still\", \"nonetheless\",\n",
        "    \"eventually\", \"usually\", \"generally\", \"specifically\", \"particularly\",\n",
        "    \"simply\", \"quickly\", \"slowly\", \"rapidly\", \"always\", \"sometimes\",\n",
        "    \"often\", \"rarely\", \"seldom\", \"hardly\", \"nearly\", \"barely\", \"easily\"\n",
        "])\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Fichier des g√®nes\n",
        "gene_file = \"/content/drive/MyDrive/hafsa_nlp_projet/unique_gene_symbols.txt\"\n",
        "with open(gene_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    gene_list = set(line.strip().upper() for line in f if line.strip())\n",
        "\n",
        "def extract_genes(text, gene_set):\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.upper())\n",
        "    return {token for token in tokens if token in gene_set and token.lower() not in stop_words}\n",
        "\n",
        "# Fichier des prot√©ines\n",
        "protein_file = \"/content/drive/MyDrive/hafsa_nlp_projet/noms_proteines.txt\"\n",
        "with open(protein_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    protein_names = set(line.strip().lower() for line in f if line.strip())\n",
        "\n",
        "def extract_proteins(text):\n",
        "    valid = set()\n",
        "    matches1 = re.findall(r'\\bprotein\\s+(\\w+)', text, re.IGNORECASE)\n",
        "    matches2 = re.findall(r'(\\w+)\\s+protein\\b', text, re.IGNORECASE)\n",
        "    for m in matches1 + matches2:\n",
        "        if m.lower() not in stop_words:\n",
        "            valid.add(m.lower())\n",
        "    from_list = {p for p in protein_names if p in text.lower() and p not in stop_words}\n",
        "    return valid.union(from_list)\n",
        "\n",
        "# Fichier des maladies\n",
        "keywords = [\"disease\", \"cancer\", \"disorder\", \"syndrome\", \"infection\", \"illness\", \"condition\",\n",
        "            \"injury\", \"tumor\", \"tumour\", \"carcinoma\", \"neoplasm\", \"pathology\", \"autoimmune\",\n",
        "            \"inflammatory\", \"genetic\", \"viral\", \"bacterial\", \"chronic\", \"arthritis\", \"diabetes\"]\n",
        "disease_pattern = re.compile(r\"\\b(?:%s)\\b\" % \"|\".join(re.escape(k) for k in keywords), re.IGNORECASE)\n",
        "def extract_diseases(text):\n",
        "    return {m for m in disease_pattern.findall(text) if m.lower() not in stop_words}\n",
        "\n",
        "# Fichier des sympt√¥mes\n",
        "symptom_file = \"/content/drive/MyDrive/hafsa_nlp_projet/symptoms_list_merged.txt\"\n",
        "with open(symptom_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    symptoms = set(line.strip().lower() for line in f if line.strip())\n",
        "\n",
        "def extract_symptoms(text):\n",
        "    return {s for s in symptoms if s in text.lower() and s not in stop_words}\n"
      ],
      "metadata": {
        "id": "qVA-LzUqXT3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les abstracts\n",
        "df = pd.read_csv(\"/content/corpus_medical.csv\")\n",
        "\n",
        "# V√©rifie qu'il y a bien une colonne \"abstract\"\n",
        "if \"Abstract\" not in df.columns:\n",
        "    raise ValueError(\"Le fichier doit contenir une colonne 'abstract'.\")\n",
        "\n",
        "# Appliquer le nettoyage + extraction des entit√©s √† chaque ligne\n",
        "df[\"cleaned\"] = df[\"Abstract\"].apply(clean_text)\n",
        "df[\"genes\"] = df[\"cleaned\"].apply(lambda text: list(extract_genes(text, gene_list)))\n",
        "df[\"proteins\"] = df[\"cleaned\"].apply(extract_proteins)\n",
        "df[\"diseases\"] = df[\"cleaned\"].apply(extract_diseases)\n",
        "df[\"symptoms\"] = df[\"cleaned\"].apply(extract_symptoms)\n",
        "\n",
        "# Sauvegarder dans un nouveau fichier CSV\n",
        "df.to_csv(\"/content/abstracts07_06_annotated_custom.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Annotation termin√©e. R√©sultat enregistr√© dans 'abstracts_annotated_custom.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hm0xVHPXTz-",
        "outputId": "48e3e1d7-a5bf-4fd0-b5df-fc68e5fff682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Annotation termin√©e. R√©sultat enregistr√© dans 'abstracts_annotated_custom.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "\n",
        "# Charger les donn√©es\n",
        "df = pd.read_csv(\"/content/abstracts07_06_annotated_custom.csv\")\n",
        "\n",
        "# Fonction de transformation en BIO\n",
        "def annotate_bio(text, gene_list, protein_list, disease_list, symptom_list):\n",
        "    words = text.split()\n",
        "    labels = [\"O\"] * len(words)\n",
        "\n",
        "    # Fusionner toutes les entit√©s avec leur label\n",
        "    entity_map = []\n",
        "    for term in gene_list:\n",
        "        entity_map.append((term.lower(), \"GENE\"))\n",
        "    for term in protein_list:\n",
        "        entity_map.append((term.lower(), \"PROTEIN\"))\n",
        "    for term in disease_list:\n",
        "        entity_map.append((term.lower(), \"DISEASE\"))\n",
        "    for term in symptom_list:\n",
        "        entity_map.append((term.lower(), \"SYMPTOM\"))\n",
        "\n",
        "    # Annoter le texte mot √† mot\n",
        "    for entity, label in entity_map:\n",
        "        entity_words = entity.split()\n",
        "        entity_len = len(entity_words)\n",
        "\n",
        "        for i in range(len(words) - entity_len + 1):\n",
        "            window = words[i:i+entity_len]\n",
        "            if [w.lower() for w in window] == entity_words:\n",
        "                labels[i] = f\"B-{label}\"\n",
        "                for j in range(1, entity_len):\n",
        "                    labels[i+j] = f\"I-{label}\"\n",
        "\n",
        "    return list(zip(words, labels))\n",
        "\n",
        "# Fonction utilitaire pour transformer les cha√Ænes en listes\n",
        "def safe_list(val):\n",
        "    if pd.isna(val) or val == '':\n",
        "        return []\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    try:\n",
        "        return ast.literal_eval(val)\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "# Application √† tout le DataFrame\n",
        "bio_data = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    text = row['Abstract']\n",
        "    genes = safe_list(row.get(\"genes\", []))\n",
        "    proteins = safe_list(row.get(\"proteins\", []))\n",
        "    diseases = safe_list(row.get(\"diseases\", []))\n",
        "    symptoms = safe_list(row.get(\"symptoms\", []))\n",
        "\n",
        "    bio_tokens = annotate_bio(text, genes, proteins, diseases, symptoms)\n",
        "\n",
        "    for word, label in bio_tokens:\n",
        "        bio_data.append({\n",
        "            \"token\": word,\n",
        "            \"label\": label,\n",
        "            \"abstract_id\": idx\n",
        "        })\n",
        "\n",
        "# Sauvegarde en CSV\n",
        "bio_df = pd.DataFrame(bio_data)\n",
        "bio_df.to_csv(\"/content/abstracts_bio_format.csv\", index=False)\n",
        "\n",
        "print(\"‚úÖ Fichier BIO g√©n√©r√© : /content/abstracts_bio_format.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCm-RmmCXiMA",
        "outputId": "d463e7ee-d1be-4063-d820-2566810655e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fichier BIO g√©n√©r√© : /content/abstracts_bio_format.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le fichier BIO\n",
        "bio_df = pd.read_csv(\"/content/abstracts_bio_format.csv\")\n",
        "\n",
        "# Compter les occurrences de chaque label\n",
        "label_counts = bio_df['label'].value_counts()\n",
        "\n",
        "# Afficher les r√©sultats\n",
        "print(\"üìä Nombre d'occurrences par label :\")\n",
        "print(label_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN3DmBXOehwg",
        "outputId": "9ddade7d-4362-4625-b73a-8141f15fee98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Nombre d'occurrences par label :\n",
            "label\n",
            "O            137750\n",
            "B-DISEASE      1084\n",
            "B-PROTEIN       853\n",
            "B-GENE          349\n",
            "I-PROTEIN        66\n",
            "B-SYMPTOM        30\n",
            "I-SYMPTOM         3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/abstracts07_06_annotated_custom.csv\")\n",
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3N8PFAsXiIm",
        "outputId": "bd8a65fc-cf9e-4e7b-f257-79564b7d5064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PMID', 'Category', 'Abstract', 'cleaned', 'genes', 'proteins', 'diseases', 'symptoms']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger les annotations\n",
        "df = pd.read_csv(\"/content/abstracts07_06_annotated_custom.csv\")\n",
        "\n",
        "# Initialiser les compteurs\n",
        "total_genes = 0\n",
        "total_proteins = 0\n",
        "total_diseases = 0\n",
        "total_symptoms = 0\n",
        "\n",
        "# Fonction pour compter les entit√©s dans une cellule\n",
        "def count_entities(cell):\n",
        "    if pd.isna(cell) or cell.strip() == \"\":\n",
        "        return 0\n",
        "    return len(cell.split(','))\n",
        "\n",
        "# Compter pour chaque ligne\n",
        "for _, row in df.iterrows():\n",
        "    total_genes += count_entities(row['genes'])\n",
        "    total_proteins += count_entities(row['proteins'])\n",
        "    total_diseases += count_entities(row['diseases'])\n",
        "    total_symptoms += count_entities(row['symptoms'])\n",
        "\n",
        "# R√©sultats\n",
        "print(\"üß¨ Nombre total de g√®nes :\", total_genes)\n",
        "print(\"üß™ Nombre total de prot√©ines :\", total_proteins)\n",
        "print(\"ü¶† Nombre total de maladies :\", total_diseases)\n",
        "print(\"ü§í Nombre total de sympt√¥mes :\", total_symptoms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQflD1EIXiG-",
        "outputId": "cf718d44-2cd8-4ef9-f266-77f8d0f4feda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß¨ Nombre total de g√®nes : 458\n",
            "üß™ Nombre total de prot√©ines : 815\n",
            "ü¶† Nombre total de maladies : 630\n",
            "ü§í Nombre total de sympt√¥mes : 331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger le fichier original\n",
        "df = pd.read_csv(\"/content/abstracts07_06_annotated_custom.csv\")\n",
        "\n",
        "# 80% entra√Ænement, 10% validation, 10% test\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Sauvegarder les splits\n",
        "train_df.to_csv(\"/content/abstracts07_06_train.csv\", index=False)\n",
        "val_df.to_csv(\"/content/abstracts07_06_val.csv\", index=False)\n",
        "test_df.to_csv(\"/content/abstracts07_06_test.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Split termin√© :\")\n",
        "print(f\"- Entra√Ænement : {len(train_df)} lignes\")\n",
        "print(f\"- Validation   : {len(val_df)} lignes\")\n",
        "print(f\"- Test         : {len(test_df)} lignes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ-m05bwXiEu",
        "outputId": "3db3c731-dc7d-4941-bf28-cb84fcb8b14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Split termin√© :\n",
            "- Entra√Ænement : 256 lignes\n",
            "- Validation   : 32 lignes\n",
            "- Test         : 32 lignes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Premiere Model"
      ],
      "metadata": {
        "id": "l_0pS5TLUN68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/content/abstracts07_06_train.csv\")\n",
        "val_df   = pd.read_csv(\"/content/abstracts07_06_val.csv\")\n",
        "test_df  = pd.read_csv(\"/content/abstracts07_06_test.csv\")\n"
      ],
      "metadata": {
        "id": "YsKvEcWoXiC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pS2lR58hJyi",
        "outputId": "8e4395e9-f816-4535-b302-30e5aed0cf77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "\n",
        "# Download 'punkt' for word_tokenize (already done, but good practice to keep)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the specific 'punkt_tab' resource mentioned in the error\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# üìå Fonction principale pour annotation BIO\n",
        "def annotate_bio(df, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for _, row in df.iterrows():\n",
        "            abstract = row['cleaned']  # ‚úÖ colonne contenant le texte nettoy√©\n",
        "\n",
        "            # Assurez-vous que la colonne 'cleaned' n'est pas vide ou None\n",
        "            if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "                 continue # Ignore les lignes sans texte nettoy√©\n",
        "\n",
        "            try:\n",
        "                # Utiliser un ensemble vide si la colonne n'existe pas\n",
        "                # ou si l'√©valuation √©choue\n",
        "                genes = set(ast.literal_eval(row.get('genes', '[]')))\n",
        "                proteins = set(ast.literal_eval(row.get('proteins', '[]')))\n",
        "                diseases = set(ast.literal_eval(row.get('diseases', '[]')))\n",
        "                symptoms = set(ast.literal_eval(row.get('symptoms', '[]')))\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur parsing √† la ligne {row.name} : {e}\")\n",
        "                continue\n",
        "\n",
        "            # G√©rer les cas o√π abstract pourrait √™tre None ou non-string\n",
        "            if not isinstance(abstract, str):\n",
        "                 print(f\"Ligne {row.name} a un type d'abstract inattendu: {type(abstract)}\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            tokens = word_tokenize(abstract)\n",
        "\n",
        "            for token in tokens:\n",
        "                tag = \"O\"\n",
        "                token_lower = token.lower()\n",
        "                token_upper = token.upper()\n",
        "\n",
        "                # It√©rer sur les ensembles d'entit√©s\n",
        "                if token_upper in genes:\n",
        "                    tag = \"B-GENE\"\n",
        "                elif token_lower in proteins:\n",
        "                    tag = \"B-PROTEIN\"\n",
        "                elif token_lower in diseases:\n",
        "                    tag = \"B-DISEASE\"\n",
        "                elif token_lower in symptoms:\n",
        "                    tag = \"B-SYMPTOM\"\n",
        "\n",
        "                fout.write(f\"{token} {tag}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Fichier BIO sauvegard√© dans : {output_path}\")\n",
        "\n",
        "# üîÑ Chargement des fichiers\n",
        "train_df = pd.read_csv(\"/content/abstracts07_06_train.csv\")\n",
        "val_df   = pd.read_csv(\"/content/abstracts07_06_val.csv\")\n",
        "test_df  = pd.read_csv(\"/content/abstracts07_06_test.csv\")\n",
        "\n",
        "# üìù Application\n",
        "annotate_bio(train_df, \"/content/abstracts07_06_bio_train.bio\")\n",
        "annotate_bio(val_df, \"/content/abstracts07_06_bio_val.bio\")\n",
        "annotate_bio(test_df, \"/content/abstracts07_06_bio_test.bio\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7BM7V7Ih5nm",
        "outputId": "907e9c55-be37-4c86-87fd-a0f6df5bd76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_train.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_val.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_test.bio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Elle ignore les tokens \"O\" dans les labels pendant l'entra√Ænement.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# üìå Mod√®le utilis√© (BioBERT)\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# üè∑Ô∏è √âtiquettes\n",
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "\n",
        "# üìÑ Lire un fichier BIO et l'organiser par phrase\n",
        "def read_bio_file(filepath):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_tokens = []\n",
        "    current_labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == '':\n",
        "                if current_tokens:\n",
        "                    sentences.append(current_tokens)\n",
        "                    labels.append(current_labels)\n",
        "                    current_tokens = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                splits = line.strip().split()\n",
        "                if len(splits) == 2:\n",
        "                    token, tag = splits\n",
        "                    current_tokens.append(token)\n",
        "                    current_labels.append(tag)\n",
        "\n",
        "        # Ajouter la derni√®re phrase si non vide\n",
        "        if current_tokens:\n",
        "            sentences.append(current_tokens)\n",
        "            labels.append(current_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "# ‚úÇÔ∏è Tokenisation + alignement des labels avec gestion du max_length\n",
        "#    et exclusion des tokens \"O\" (ignor√©s avec -100)\n",
        "def tokenize_and_align(sentences, tags, tokenizer):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        sentences,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                tag = label[word_idx]\n",
        "                label_ids.append(label2id[tag] if tag != \"O\" else -100)\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# üìÅ Pr√©parer un Dataset HuggingFace depuis un fichier BIO\n",
        "def prepare_dataset_from_bio(filepath):\n",
        "    sents, tags = read_bio_file(filepath)\n",
        "    tokenized = tokenize_and_align(sents, tags, tokenizer)\n",
        "    return Dataset.from_dict(tokenized)\n",
        "\n",
        "\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "train_dataset = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_train.bio\")\n",
        "val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val.bio\")\n",
        "test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test.bio\")\n",
        "\n",
        "# üîç V√©rification\n",
        "print(\"‚úÖ Datasets cr√©√©s :\")\n",
        "print(\"Exemple train_dataset :\")\n",
        "print(train_dataset[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "5e47b82132ae45c5a7885549bd129481",
            "aa01a4354a1a40b1a3e43d71824f4f23",
            "83d7aea1126043a1a633895d7dee6b33",
            "3ddd05b58b354b48a78c5d3dff8eb5ed",
            "c3008be2e9fd45dea255457984ca5c07",
            "b89d949cd6fa4a9e8c5f707d49d4d639",
            "2305a10f125947f6b4e2c5ca59125da2",
            "138925f6cf354a2492760073fd4f3360",
            "328a33d62d8840a388ce191626d751fa",
            "4b4ab68e00504436ac6648f7398a164f",
            "22a2387e2eb74876ae7ca82ad0acc3a4",
            "24a722f799014450ab2f6e029a0dd2ec",
            "56dac657eba44020aac66e44f23b9685",
            "022b3f98e8624576a99b191c29e2c158",
            "6384df2a6fe74f07950afb71f3638984",
            "d8d3ea612f6f4773b6d4b1e024744c85",
            "541b25b3f4924e009e1701fb9aa9d38f",
            "bc6c8426bd9a4be692574f88f6684335",
            "545e966959ed4a1a93ee6454e7c627d8",
            "6320a4fb998540119f7be776b4c2c6b1",
            "8d18369616244492a74220e9917a000b",
            "3fdaf3bbcc5341fca341cf4fefcce10c"
          ]
        },
        "id": "wy8oacm8hAB9",
        "outputId": "45a3e801-14ee-47c2-8969-60e308911336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e47b82132ae45c5a7885549bd129481"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24a722f799014450ab2f6e029a0dd2ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datasets cr√©√©s :\n",
            "Exemple train_dataset :\n",
            "{'input_ids': [101, 122, 24443, 22572, 4060, 2496, 1161, 17881, 1571, 14516, 1643, 122, 16222, 1571, 3236, 23117, 22997, 9465, 1275, 7393, 1545, 179, 170, 2599, 17881, 1571, 3236, 23117, 22997, 174, 16091, 1830, 17881, 1571, 1336, 1695, 15139, 17599, 2087, 7535, 2386, 1596, 11140, 3919, 4164, 6099, 1751, 11432, 2022, 3023, 3507, 17960, 20942, 181, 1182, 177, 122, 11437, 1162, 194, 123, 192, 1358, 194, 124, 11019, 1186, 194, 125, 181, 19009, 192, 126, 195, 22235, 187, 127, 175, 14429, 193, 128, 5871, 1186, 187, 129, 2351, 1869, 122, 1278, 22759, 5144, 1161, 2657, 2755, 1131, 15449, 2118, 6745, 11964, 1477, 5144, 1161, 2853, 12844, 4807, 21718, 2605, 16198, 8117, 1278, 1470, 2332, 2364, 2657, 2755, 1129, 23784, 2118, 6087, 1545, 1580, 5144, 1161, 2853, 3653, 13347, 1654, 1148, 2657, 2057, 5144, 6420, 185, 1742, 1704, 2704, 1129, 23784, 2118, 1620, 1604, 24239, 5144, 1161, 4828, 4134, 17801, 24606, 24766, 1545, 1580, 1580, 1559, 19207, 3254, 123, 2853, 12844, 4807, 21718, 2605, 16198, 8117, 1278, 1470, 2332, 2364, 2657, 2755, 1129, 23784, 2118, 6087, 1545, 1580, 5144, 1161, 4828, 4134, 11437, 2254, 6094, 11964, 1495, 17594, 14746, 3254, 124, 2853, 12844, 4807, 21718, 2605, 16198, 8117, 1278, 1470, 2332, 2364, 2657, 2755, 1129, 23784, 2118, 6087, 1545, 1580, 5144, 1161, 4828, 4134, 192, 20257, 1182, 7629, 23124, 19207, 3254, 125, 2853, 12844, 4807, 21718, 2605, 16198, 8117, 1278, 1470, 2332, 2364, 2657, 2755, 1129, 23784, 2118, 6087, 1545, 1580, 5144, 1161, 4828, 4134, 172, 1183, 1183, 3457, 14402, 13601, 5048, 1358, 172, 1179, 126, 5144, 6420, 185, 1742, 2057, 3653, 1654, 13347, 1129, 23784, 2118, 6087, 1559, 1475, 5144, 1161, 4828, 4134, 17128, 1559, 22392, 1571, 10973, 1580, 19207, 3254, 127, 5144, 6420, 185, 1742, 2057, 3653, 1654, 13347, 1129, 23784, 2118, 6087, 1559, 1475, 5144, 1161, 4828, 4134, 195, 22235, 16822, 1777, 1186, 16382, 1604, 1527, 19207, 3254, 128, 1278, 22759, 5144, 1161, 2657, 2755, 1131, 15449, 2118, 6745, 11964, 1477, 5144, 1161, 4828, 4134, 3262, 9650, 1813, 14491, 3254, 129, 2853, 12844, 4807, 21718, 2605, 16198, 8117, 1278, 1470, 2332, 2364, 2657, 2755, 1129, 23784, 2118, 6087, 1545, 1580, 5144, 1161, 4828, 4134, 5871, 1186, 14402, 13601, 5048, 1358, 172, 1179, 3582, 2793, 4265, 2819, 3023, 3507, 17960, 20942, 182, 26466, 174, 15792, 1161, 20942, 12976, 14844, 1444, 2967, 1775, 11432, 7951, 1954, 4069, 2960, 11137, 19648, 9117, 5871, 20900, 1116, 8010, 1654, 9100, 2609, 11106, 4265, 10900, 3268, 2361, 4884, 1176, 1842, 1159, 25220, 21176, 6530, 4129, 3943, 186, 1643, 1665, 1197, 24034, 18890, 9100, 2609, 1877, 1496, 8087, 12864, 9517, 7623, 4675, 5420, 15139, 6099, 2616, 3903, 3442, 2967, 1775, 1768, 11432, 3023, 3507, 17960, 20942, 11774, 1834, 4134, 2898, 4433, 8010, 1116, 9100, 2609, 11106, 2686, 2025, 3014, 6099, 27349, 1665, 5190, 16026, 3442, 1982, 1606, 2526, 1359, 20866, 23743, 3204, 1714, 2063, 1439, 123, 11241, 2886, 17599, 2087, 7535, 2386, 1596, 11451, 6576, 7812, 22060, 1110, 12858, 1200, 7435, 1821, 1643, 17489, 11140, 2815, 3689, 1768, 23660, 1895, 15139, 11432, 3442, 12201, 17599, 2087, 7535, 2386, 1596, 11140, 3919, 4164, 3919, 4164, 3643, 19648, 11432, 2022, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vulcEccl319",
        "outputId": "d4e201e9-b5d6-4a9b-a51c-295f51efa346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=58e0ae9d41eb3d39f695bf09fb1c0dc94e601e6c398dfc83775a9cb2bf3c9623\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# ‚úÖ Charger le mod√®le pr√©-entra√Æn√©\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# üìä M√©triques d‚Äô√©valuation\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "    }\n",
        "\n",
        "# üîß Arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_biobert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# üöÄ Entra√Ænement avec Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "3xUf0kDShAAA",
        "outputId": "8a531cf9-f8fc-41e9-db74-2254000d461f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-24-3145e7aac804>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhafsaikram31\u001b[0m (\u001b[33mhafsaikram31-cadi-ayyad\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250609_102608-iucpzgxi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hafsaikram31-cadi-ayyad/huggingface/runs/iucpzgxi' target=\"_blank\">./ner_biobert</a></strong> to <a href='https://wandb.ai/hafsaikram31-cadi-ayyad/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hafsaikram31-cadi-ayyad/huggingface' target=\"_blank\">https://wandb.ai/hafsaikram31-cadi-ayyad/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hafsaikram31-cadi-ayyad/huggingface/runs/iucpzgxi' target=\"_blank\">https://wandb.ai/hafsaikram31-cadi-ayyad/huggingface/runs/iucpzgxi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 01:42, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.560000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.173500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=128, training_loss=0.30564939603209496, metrics={'train_runtime': 117.4076, 'train_samples_per_second': 8.722, 'train_steps_per_second': 1.09, 'total_flos': 267575136092160.0, 'train_loss': 0.30564939603209496, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âvaluer le mod√®le sur le jeu de validation\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Affichage des m√©triques principales\n",
        "print(\"üìä R√©sultats de l'√©valuation :\")\n",
        "print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "p6-sxTUug_7R",
        "outputId": "fafd90da-77c5-44f9-bd6a-5e17c7c35c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats de l'√©valuation :\n",
            "Accuracy : 0.9504\n",
            "F1-score : 0.9504\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.98      0.97      0.97        94\n",
            "        GENE       0.88      0.98      0.93        45\n",
            "     PROTEIN       0.96      0.93      0.95       102\n",
            "     SYMPTOM       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.95      0.95      0.95       242\n",
            "   macro avg       0.70      0.72      0.71       242\n",
            "weighted avg       0.95      0.95      0.95       242\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä √âvaluation sur le jeu de test\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# üìã Affichage des m√©triques\n",
        "print(\"üìä R√©sultats sur le jeu de test :\")\n",
        "print(f\"Accuracy : {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {test_results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(test_results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "emdAoT-xlb_5",
        "outputId": "f730ec3b-757e-44e1-8454-9134fa405fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 01:24]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats sur le jeu de test :\n",
            "Accuracy : 0.9648\n",
            "F1-score : 0.9648\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.97      1.00      0.99       101\n",
            "        GENE       0.91      0.94      0.92        52\n",
            "     PROTEIN       1.00      0.95      0.97        73\n",
            "     SYMPTOM       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.96      0.96      0.96       227\n",
            "   macro avg       0.72      0.72      0.72       227\n",
            "weighted avg       0.96      0.96      0.96       227\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = \"/content\"\n",
        "target_dir = \"/content/drive/MyDrive/NLP_dernier_modification_09_06\"\n",
        "\n",
        "# Cr√©er le dossier cible s‚Äôil n‚Äôexiste pas\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Parcourir tous les fichiers/dossiers dans /content sauf /content/drive\n",
        "for item in os.listdir(source_dir):\n",
        "    src_path = os.path.join(source_dir, item)\n",
        "    dst_path = os.path.join(target_dir, item)\n",
        "\n",
        "    if item == \"drive\":\n",
        "        continue  # ‚ö†Ô∏è Ignorer le dossier Google Drive\n",
        "\n",
        "    try:\n",
        "        if os.path.isdir(src_path):\n",
        "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur en copiant {item} : {e}\")\n",
        "\n",
        "print(f\"‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : {target_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kUijvWmlb8f",
        "outputId": "433bad24-0bcf-485a-f97d-090b49d1387b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : /content/drive/MyDrive/NLP_dernier_modification_09_06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test d'abilation sur data"
      ],
      "metadata": {
        "id": "qPq3x1vpQ8Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Charger le fichier original\n",
        "df = pd.read_csv(\"/content/abstracts07_06_annotated_custom.csv\")\n",
        "\n",
        "# Diff√©rentes configurations √† tester\n",
        "configurations = [\n",
        "    # (train%, val%, test%, nom)\n",
        "    (0.8, 0.1, 0.1, \"80_10_10\"),     # Original\n",
        "    (0.7, 0.15, 0.15, \"70_15_15\"),   # Plus √©quilibr√©\n",
        "    (0.9, 0.05, 0.05, \"90_5_5\"),     # Plus d'entra√Ænement\n",
        "    (0.6, 0.2, 0.2, \"60_20_20\"),     # Plus de validation/test\n",
        "    (0.75, 0.125, 0.125, \"75_12.5_12.5\"), # Interm√©diaire\n",
        "]\n",
        "\n",
        "print(f\"üìä Dataset total : {len(df)} lignes\\n\")\n",
        "\n",
        "for train_pct, val_pct, test_pct, name in configurations:\n",
        "    print(f\"üîÑ Configuration {name} (Train: {train_pct:.1%}, Val: {val_pct:.1%}, Test: {test_pct:.1%})\")\n",
        "\n",
        "    # Premier split : train vs (val+test)\n",
        "    train_df, temp_df = train_test_split(df, test_size=(val_pct + test_pct), random_state=42)\n",
        "\n",
        "    # Deuxi√®me split : val vs test\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=(test_pct/(val_pct + test_pct)), random_state=42)\n",
        "\n",
        "    # Sauvegarder avec suffixe\n",
        "    train_df.to_csv(f\"/content/abstracts07_06_train_{name}.csv\", index=False)\n",
        "    val_df.to_csv(f\"/content/abstracts07_06_val_{name}.csv\", index=False)\n",
        "    test_df.to_csv(f\"/content/abstracts07_06_test_{name}.csv\", index=False)\n",
        "\n",
        "    print(f\"  ‚úÖ Entra√Ænement : {len(train_df)} lignes ({len(train_df)/len(df):.1%})\")\n",
        "    print(f\"  ‚úÖ Validation   : {len(val_df)} lignes ({len(val_df)/len(df):.1%})\")\n",
        "    print(f\"  ‚úÖ Test         : {len(test_df)} lignes ({len(test_df)/len(df):.1%})\")\n",
        "    print(f\"  üìÅ Fichiers sauvegard√©s avec suffixe '_{name}'\\n\")\n",
        "\n",
        "print(\"‚úÖ Tous les splits termin√©s !\")\n",
        "print(\"\\nFichiers cr√©√©s :\")\n",
        "for _, _, _, name in configurations:\n",
        "    print(f\"  - abstracts07_06_train_{name}.csv\")\n",
        "    print(f\"  - abstracts07_06_val_{name}.csv\")\n",
        "    print(f\"  - abstracts07_06_test_{name}.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PGc_RLxRA37",
        "outputId": "f614a0d7-8ac3-47bf-be15-c780f6498464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Dataset total : 320 lignes\n",
            "\n",
            "üîÑ Configuration 80_10_10 (Train: 80.0%, Val: 10.0%, Test: 10.0%)\n",
            "  ‚úÖ Entra√Ænement : 256 lignes (80.0%)\n",
            "  ‚úÖ Validation   : 32 lignes (10.0%)\n",
            "  ‚úÖ Test         : 32 lignes (10.0%)\n",
            "  üìÅ Fichiers sauvegard√©s avec suffixe '_80_10_10'\n",
            "\n",
            "üîÑ Configuration 70_15_15 (Train: 70.0%, Val: 15.0%, Test: 15.0%)\n",
            "  ‚úÖ Entra√Ænement : 224 lignes (70.0%)\n",
            "  ‚úÖ Validation   : 48 lignes (15.0%)\n",
            "  ‚úÖ Test         : 48 lignes (15.0%)\n",
            "  üìÅ Fichiers sauvegard√©s avec suffixe '_70_15_15'\n",
            "\n",
            "üîÑ Configuration 90_5_5 (Train: 90.0%, Val: 5.0%, Test: 5.0%)\n",
            "  ‚úÖ Entra√Ænement : 288 lignes (90.0%)\n",
            "  ‚úÖ Validation   : 16 lignes (5.0%)\n",
            "  ‚úÖ Test         : 16 lignes (5.0%)\n",
            "  üìÅ Fichiers sauvegard√©s avec suffixe '_90_5_5'\n",
            "\n",
            "üîÑ Configuration 60_20_20 (Train: 60.0%, Val: 20.0%, Test: 20.0%)\n",
            "  ‚úÖ Entra√Ænement : 192 lignes (60.0%)\n",
            "  ‚úÖ Validation   : 64 lignes (20.0%)\n",
            "  ‚úÖ Test         : 64 lignes (20.0%)\n",
            "  üìÅ Fichiers sauvegard√©s avec suffixe '_60_20_20'\n",
            "\n",
            "üîÑ Configuration 75_12.5_12.5 (Train: 75.0%, Val: 12.5%, Test: 12.5%)\n",
            "  ‚úÖ Entra√Ænement : 240 lignes (75.0%)\n",
            "  ‚úÖ Validation   : 40 lignes (12.5%)\n",
            "  ‚úÖ Test         : 40 lignes (12.5%)\n",
            "  üìÅ Fichiers sauvegard√©s avec suffixe '_75_12.5_12.5'\n",
            "\n",
            "‚úÖ Tous les splits termin√©s !\n",
            "\n",
            "Fichiers cr√©√©s :\n",
            "  - abstracts07_06_train_80_10_10.csv\n",
            "  - abstracts07_06_val_80_10_10.csv\n",
            "  - abstracts07_06_test_80_10_10.csv\n",
            "  - abstracts07_06_train_70_15_15.csv\n",
            "  - abstracts07_06_val_70_15_15.csv\n",
            "  - abstracts07_06_test_70_15_15.csv\n",
            "  - abstracts07_06_train_90_5_5.csv\n",
            "  - abstracts07_06_val_90_5_5.csv\n",
            "  - abstracts07_06_test_90_5_5.csv\n",
            "  - abstracts07_06_train_60_20_20.csv\n",
            "  - abstracts07_06_val_60_20_20.csv\n",
            "  - abstracts07_06_test_60_20_20.csv\n",
            "  - abstracts07_06_train_75_12.5_12.5.csv\n",
            "  - abstracts07_06_val_75_12.5_12.5.csv\n",
            "  - abstracts07_06_test_75_12.5_12.5.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _90_5_5"
      ],
      "metadata": {
        "id": "Q6qgoe34VVx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "\n",
        "# Download 'punkt' for word_tokenize (already done, but good practice to keep)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the specific 'punkt_tab' resource mentioned in the error\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# üìå Fonction principale pour annotation BIO\n",
        "def annotate_bio(df, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for _, row in df.iterrows():\n",
        "            abstract = row['cleaned']  # ‚úÖ colonne contenant le texte nettoy√©\n",
        "\n",
        "            # Assurez-vous que la colonne 'cleaned' n'est pas vide ou None\n",
        "            if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "                 continue # Ignore les lignes sans texte nettoy√©\n",
        "\n",
        "            try:\n",
        "                # Utiliser un ensemble vide si la colonne n'existe pas\n",
        "                # ou si l'√©valuation √©choue\n",
        "                genes = set(ast.literal_eval(row.get('genes', '[]')))\n",
        "                proteins = set(ast.literal_eval(row.get('proteins', '[]')))\n",
        "                diseases = set(ast.literal_eval(row.get('diseases', '[]')))\n",
        "                symptoms = set(ast.literal_eval(row.get('symptoms', '[]')))\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur parsing √† la ligne {row.name} : {e}\")\n",
        "                continue\n",
        "\n",
        "            # G√©rer les cas o√π abstract pourrait √™tre None ou non-string\n",
        "            if not isinstance(abstract, str):\n",
        "                 print(f\"Ligne {row.name} a un type d'abstract inattendu: {type(abstract)}\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            tokens = word_tokenize(abstract)\n",
        "\n",
        "            for token in tokens:\n",
        "                tag = \"O\"\n",
        "                token_lower = token.lower()\n",
        "                token_upper = token.upper()\n",
        "\n",
        "                # It√©rer sur les ensembles d'entit√©s\n",
        "                if token_upper in genes:\n",
        "                    tag = \"B-GENE\"\n",
        "                elif token_lower in proteins:\n",
        "                    tag = \"B-PROTEIN\"\n",
        "                elif token_lower in diseases:\n",
        "                    tag = \"B-DISEASE\"\n",
        "                elif token_lower in symptoms:\n",
        "                    tag = \"B-SYMPTOM\"\n",
        "\n",
        "                fout.write(f\"{token} {tag}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Fichier BIO sauvegard√© dans : {output_path}\")\n",
        "\n",
        "# üîÑ Chargement des fichiers\n",
        "train_df = pd.read_csv(\"/content/abstracts07_06_train_90_5_5.csv\")\n",
        "val_df   = pd.read_csv(\"/content/abstracts07_06_val_90_5_5.csv\")\n",
        "test_df  = pd.read_csv(\"/content/abstracts07_06_test_90_5_5.csv\")\n",
        "\n",
        "# üìù Application\n",
        "annotate_bio(train_df, \"/content/abstracts07_06_bio_train_90_5_5.bio\")\n",
        "annotate_bio(val_df, \"/content/abstracts07_06_bio_val_90_5_5.bio\")\n",
        "annotate_bio(test_df, \"/content/abstracts07_06_bio_test_90_5_5.bio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCPZgwnPRdra",
        "outputId": "c6197d71-b567-4d15-a5cd-6bee8adb520d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_train_90_5_5.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_val_90_5_5.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_test_90_5_5.bio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Elle ignore les tokens \"O\" dans les labels pendant l'entra√Ænement.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# üìå Mod√®le utilis√© (BioBERT)\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# üè∑Ô∏è √âtiquettes\n",
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "\n",
        "# üìÑ Lire un fichier BIO et l'organiser par phrase\n",
        "def read_bio_file(filepath):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_tokens = []\n",
        "    current_labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == '':\n",
        "                if current_tokens:\n",
        "                    sentences.append(current_tokens)\n",
        "                    labels.append(current_labels)\n",
        "                    current_tokens = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                splits = line.strip().split()\n",
        "                if len(splits) == 2:\n",
        "                    token, tag = splits\n",
        "                    current_tokens.append(token)\n",
        "                    current_labels.append(tag)\n",
        "\n",
        "        # Ajouter la derni√®re phrase si non vide\n",
        "        if current_tokens:\n",
        "            sentences.append(current_tokens)\n",
        "            labels.append(current_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "# ‚úÇÔ∏è Tokenisation + alignement des labels avec gestion du max_length\n",
        "#    et exclusion des tokens \"O\" (ignor√©s avec -100)\n",
        "def tokenize_and_align(sentences, tags, tokenizer):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        sentences,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                tag = label[word_idx]\n",
        "                label_ids.append(label2id[tag] if tag != \"O\" else -100)\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# üìÅ Pr√©parer un Dataset HuggingFace depuis un fichier BIO\n",
        "def prepare_dataset_from_bio(filepath):\n",
        "    sents, tags = read_bio_file(filepath)\n",
        "    tokenized = tokenize_and_align(sents, tags, tokenizer)\n",
        "    return Dataset.from_dict(tokenized)\n",
        "\n",
        "\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "train_dataset = prepare_dataset_from_bio(\"//content/abstracts07_06_bio_train_90_5_5.bio\")\n",
        "val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val_90_5_5.bio\")\n",
        "test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test_90_5_5.bio\")\n",
        "\n",
        "# üîç V√©rification\n",
        "print(\"‚úÖ Datasets cr√©√©s :\")\n",
        "print(\"Exemple train_dataset :\")\n",
        "print(train_dataset[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xulRwcOFSwUP",
        "outputId": "15891c28-e954-4d09-b2bf-c18de34e9526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datasets cr√©√©s :\n",
            "Exemple train_dataset :\n",
            "{'input_ids': [101, 122, 25128, 4386, 1306, 185, 7111, 1918, 2528, 1233, 17881, 1571, 179, 3488, 127, 12737, 1568, 21336, 9465, 1275, 7393, 1545, 179, 171, 1665, 1643, 17881, 1571, 12737, 1568, 21336, 3294, 3075, 5911, 2393, 1377, 1830, 1324, 1559, 2394, 3052, 26410, 13499, 3946, 2629, 16516, 6602, 15796, 3850, 172, 1548, 1643, 16236, 1394, 10645, 12104, 16042, 3773, 23220, 4993, 177, 122, 177, 1358, 193, 123, 195, 10436, 1403, 181, 124, 16358, 2118, 194, 125, 2351, 1869, 122, 2853, 1137, 1582, 15680, 4724, 188, 17204, 10390, 8281, 1234, 188, 2704, 11371, 1403, 3454, 2755, 1278, 5182, 188, 17204, 10390, 1539, 1559, 1477, 185, 1197, 5144, 1161, 1278, 1297, 2332, 8614, 175, 20257, 9513, 2755, 2598, 2815, 175, 14875, 14640, 8301, 10424, 1580, 185, 1197, 5144, 1161, 123, 1131, 19411, 10436, 2057, 13306, 3653, 1654, 13347, 1131, 19411, 10436, 8918, 4167, 21943, 4807, 1131, 19411, 10436, 4062, 18910, 10973, 185, 1197, 5144, 1161, 124, 2853, 1137, 1582, 15680, 4724, 188, 17204, 10390, 8281, 1234, 188, 2704, 11371, 1403, 3454, 2755, 1278, 5182, 188, 17204, 10390, 1539, 1559, 1477, 185, 1197, 5144, 1161, 4828, 4134, 16358, 2118, 10279, 16740, 175, 9379, 2050, 5048, 1358, 172, 1179, 125, 1278, 1297, 2332, 8614, 175, 20257, 9513, 2755, 2598, 2815, 175, 14875, 14640, 8301, 10424, 1580, 185, 1197, 5144, 1161, 4828, 4134, 173, 1197, 195, 10436, 1403, 11371, 1403, 3454, 5048, 1358, 172, 1179, 172, 1548, 1643, 16236, 1394, 18106, 22572, 5521, 12858, 5970, 3186, 16065, 1665, 3850, 3409, 1215, 7299, 4182, 1116, 16679, 1166, 7409, 19172, 6856, 16042, 5250, 23984, 15511, 5552, 2765, 185, 1204, 1665, 2686, 5199, 12104, 16042, 3773, 170, 2293, 16516, 6602, 15796, 5557, 1263, 1215, 7299, 3245, 8005, 10879, 2556, 14196, 14441, 8936, 1119, 8031, 12809, 11179, 1200, 185, 7777, 9012, 8974, 5426, 1193, 9619, 3469, 16516, 6602, 15796, 5557, 1169, 1145, 26410, 25342, 172, 1548, 1643, 16236, 1394, 10645, 170, 2293, 10311, 6978, 2606, 3655, 2025, 1276, 1256, 1822, 14759, 16516, 6602, 15796, 1169, 2773, 8115, 2603, 1769, 185, 1204, 1665, 1852, 12177, 172, 1548, 1643, 16236, 1394, 7401, 1606, 4321, 18882, 6063, 4592, 1353, 14730, 8234, 4884, 3626, 27850, 3926, 7861, 4592, 2393, 1377, 1830, 1324, 1559, 2747, 4010, 16516, 6602, 15796, 5565, 13251, 27466, 7836, 4869, 7160, 2393, 1377, 1830, 1324, 1559, 1260, 7136, 2116, 5958, 6986, 2765, 8115, 2603, 1852, 12177, 172, 1548, 1643, 16236, 1394, 7401, 1276, 16516, 6602, 15796, 3252, 1260, 7136, 3052, 2393, 1377, 1830, 1324, 1559, 14105, 16042, 17356, 172, 1548, 1643, 16236, 1394, 10645, 170, 2293, 2905, 2554, 5401, 2393, 1377, 1830, 1324, 1559, 2747, 4010, 16516, 6602, 15796, 5557, 2393, 1377, 1830, 1324, 1559, 27558, 1116, 3209, 3850, 4765, 26410, 13499, 2116, 172, 1548, 1643, 16236, 1394, 10645, 170, 2293, 11409, 17881, 1571, 1502, 1950, 15339, 1107, 1665, 9465, 1275, 7393, 1545, 179, 171, 1665, 1643, 17881, 1571, 12737, 1568, 21336, 9852, 2386, 27993, 1604, 12882, 23249, 4139, 2199, 4195, 11156, 6259, 2199, 5752, 14197, 1227, 6259, 2798, 4740, 2357, 6085, 1691, 2933, 1250, 2103, 2526, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, 2, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# ‚úÖ Charger le mod√®le pr√©-entra√Æn√©\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# üìä M√©triques d‚Äô√©valuation\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "    }\n",
        "\n",
        "# üîß Arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_biobert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# üöÄ Entra√Ænement avec Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "d22G2TdZS3eZ",
        "outputId": "46ebfcbb-8114-419e-b23b-7d78cbd2a9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-38-3145e7aac804>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [144/144 01:58, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.558500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.124400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=144, training_loss=0.2566818479034636, metrics={'train_runtime': 119.0834, 'train_samples_per_second': 9.674, 'train_steps_per_second': 1.209, 'total_flos': 301022028103680.0, 'train_loss': 0.2566818479034636, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âvaluer le mod√®le sur le jeu de validation\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Affichage des m√©triques principales\n",
        "print(\"üìä R√©sultats de l'√©valuation :\")\n",
        "print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "wvL5uSnLS7Q0",
        "outputId": "ce6ff25f-751e-4389-8a11-621c6b59f672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats de l'√©valuation :\n",
            "Accuracy : 1.0000\n",
            "F1-score : 1.0000\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       1.00      1.00      1.00        49\n",
            "        GENE       1.00      1.00      1.00        38\n",
            "     PROTEIN       1.00      1.00      1.00         5\n",
            "     SYMPTOM       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       1.00      1.00      1.00        93\n",
            "   macro avg       1.00      1.00      1.00        93\n",
            "weighted avg       1.00      1.00      1.00        93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä √âvaluation sur le jeu de test\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# üìã Affichage des m√©triques\n",
        "print(\"üìä R√©sultats sur le jeu de test :\")\n",
        "print(f\"Accuracy : {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {test_results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(test_results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "rpdnUlhgS_p8",
        "outputId": "481ba435-72a5-4251-8b9d-6c582881ba91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:10]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats sur le jeu de test :\n",
            "Accuracy : 0.9417\n",
            "F1-score : 0.9417\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.97      1.00      0.98        61\n",
            "        GENE       0.96      0.83      0.89        29\n",
            "     PROTEIN       0.80      0.92      0.86        13\n",
            "\n",
            "   micro avg       0.94      0.94      0.94       103\n",
            "   macro avg       0.91      0.92      0.91       103\n",
            "weighted avg       0.94      0.94      0.94       103\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = \"/content\"\n",
        "target_dir = \"/content/drive/MyDrive/NLP_dernier_modification_09_06_90_5_5\"\n",
        "\n",
        "# Cr√©er le dossier cible s‚Äôil n‚Äôexiste pas\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Parcourir tous les fichiers/dossiers dans /content sauf /content/drive\n",
        "for item in os.listdir(source_dir):\n",
        "    src_path = os.path.join(source_dir, item)\n",
        "    dst_path = os.path.join(target_dir, item)\n",
        "\n",
        "    if item == \"drive\":\n",
        "        continue  # ‚ö†Ô∏è Ignorer le dossier Google Drive\n",
        "\n",
        "    try:\n",
        "        if os.path.isdir(src_path):\n",
        "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur en copiant {item} : {e}\")\n",
        "\n",
        "print(f\"‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : {target_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z23Sv7FfTEvk",
        "outputId": "e3256f24-4261-49c3-a6ae-34994f787d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : /content/drive/MyDrive/NLP_dernier_modification_09_06_90_5_5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "n6hYfS_7STrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _70_15_15"
      ],
      "metadata": {
        "id": "xt3KEv-IVndD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "\n",
        "# Download 'punkt' for word_tokenize (already done, but good practice to keep)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the specific 'punkt_tab' resource mentioned in the error\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# üìå Fonction principale pour annotation BIO\n",
        "def annotate_bio(df, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for _, row in df.iterrows():\n",
        "            abstract = row['cleaned']  # ‚úÖ colonne contenant le texte nettoy√©\n",
        "\n",
        "            # Assurez-vous que la colonne 'cleaned' n'est pas vide ou None\n",
        "            if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "                 continue # Ignore les lignes sans texte nettoy√©\n",
        "\n",
        "            try:\n",
        "                # Utiliser un ensemble vide si la colonne n'existe pas\n",
        "                # ou si l'√©valuation √©choue\n",
        "                genes = set(ast.literal_eval(row.get('genes', '[]')))\n",
        "                proteins = set(ast.literal_eval(row.get('proteins', '[]')))\n",
        "                diseases = set(ast.literal_eval(row.get('diseases', '[]')))\n",
        "                symptoms = set(ast.literal_eval(row.get('symptoms', '[]')))\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur parsing √† la ligne {row.name} : {e}\")\n",
        "                continue\n",
        "\n",
        "            # G√©rer les cas o√π abstract pourrait √™tre None ou non-string\n",
        "            if not isinstance(abstract, str):\n",
        "                 print(f\"Ligne {row.name} a un type d'abstract inattendu: {type(abstract)}\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            tokens = word_tokenize(abstract)\n",
        "\n",
        "            for token in tokens:\n",
        "                tag = \"O\"\n",
        "                token_lower = token.lower()\n",
        "                token_upper = token.upper()\n",
        "\n",
        "                # It√©rer sur les ensembles d'entit√©s\n",
        "                if token_upper in genes:\n",
        "                    tag = \"B-GENE\"\n",
        "                elif token_lower in proteins:\n",
        "                    tag = \"B-PROTEIN\"\n",
        "                elif token_lower in diseases:\n",
        "                    tag = \"B-DISEASE\"\n",
        "                elif token_lower in symptoms:\n",
        "                    tag = \"B-SYMPTOM\"\n",
        "\n",
        "                fout.write(f\"{token} {tag}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Fichier BIO sauvegard√© dans : {output_path}\")\n",
        "\n",
        "# üîÑ Chargement des fichiers\n",
        "train_df = pd.read_csv(\"/content/abstracts07_06_train_70_15_15.csv\")\n",
        "val_df   = pd.read_csv(\"/content/abstracts07_06_val_70_15_15.csv\")\n",
        "test_df  = pd.read_csv(\"/content/abstracts07_06_test_70_15_15.csv\")\n",
        "\n",
        "# üìù Application\n",
        "annotate_bio(train_df, \"/content/abstracts07_06_bio_train_70_15_15.bio\")\n",
        "annotate_bio(val_df, \"/content/abstracts07_06_bio_val_70_15_15.bio\")\n",
        "annotate_bio(test_df, \"/content/abstracts07_06_bio_test_70_15_15.bio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDSw6OSEYYpy",
        "outputId": "36a67b33-49be-4596-9c40-efb1acf00392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_train_70_15_15.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_val_70_15_15.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_test_70_15_15.bio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Elle ignore les tokens \"O\" dans les labels pendant l'entra√Ænement.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# üìå Mod√®le utilis√© (BioBERT)\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# üè∑Ô∏è √âtiquettes\n",
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "\n",
        "# üìÑ Lire un fichier BIO et l'organiser par phrase\n",
        "def read_bio_file(filepath):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_tokens = []\n",
        "    current_labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == '':\n",
        "                if current_tokens:\n",
        "                    sentences.append(current_tokens)\n",
        "                    labels.append(current_labels)\n",
        "                    current_tokens = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                splits = line.strip().split()\n",
        "                if len(splits) == 2:\n",
        "                    token, tag = splits\n",
        "                    current_tokens.append(token)\n",
        "                    current_labels.append(tag)\n",
        "\n",
        "        # Ajouter la derni√®re phrase si non vide\n",
        "        if current_tokens:\n",
        "            sentences.append(current_tokens)\n",
        "            labels.append(current_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "# ‚úÇÔ∏è Tokenisation + alignement des labels avec gestion du max_length\n",
        "#    et exclusion des tokens \"O\" (ignor√©s avec -100)\n",
        "def tokenize_and_align(sentences, tags, tokenizer):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        sentences,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                tag = label[word_idx]\n",
        "                label_ids.append(label2id[tag] if tag != \"O\" else -100)\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# üìÅ Pr√©parer un Dataset HuggingFace depuis un fichier BIO\n",
        "def prepare_dataset_from_bio(filepath):\n",
        "    sents, tags = read_bio_file(filepath)\n",
        "    tokenized = tokenize_and_align(sents, tags, tokenizer)\n",
        "    return Dataset.from_dict(tokenized)\n",
        "\n",
        "\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "train_dataset = prepare_dataset_from_bio(\"//content/abstracts07_06_bio_train_70_15_15.bio\")\n",
        "val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val_70_15_15.bio\")\n",
        "test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test_70_15_15.bio\")\n",
        "\n",
        "# üîç V√©rification\n",
        "print(\"‚úÖ Datasets cr√©√©s :\")\n",
        "print(\"Exemple train_dataset :\")\n",
        "print(train_dataset[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4NTiOyGYh3k",
        "outputId": "727a8202-5d8f-43bd-90df-e9e183349207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datasets cr√©√©s :\n",
            "Exemple train_dataset :\n",
            "{'input_ids': [101, 122, 195, 15564, 12491, 194, 1182, 193, 4175, 195, 1161, 195, 3031, 17881, 1571, 179, 3488, 1275, 8359, 1659, 12445, 11004, 9465, 1275, 3413, 16480, 3975, 1161, 179, 172, 1179, 14541, 18202, 26303, 17881, 11049, 23117, 1475, 3135, 1580, 1604, 1477, 3772, 6600, 2272, 18250, 3246, 1558, 16798, 3621, 25575, 1958, 1664, 1353, 2765, 13093, 4182, 4420, 1606, 1407, 175, 175, 1181, 1403, 11109, 172, 1204, 3342, 5144, 6420, 11108, 1907, 5144, 6420, 6654, 181, 1182, 192, 122, 181, 1182, 179, 1324, 123, 195, 6583, 179, 1665, 122, 20049, 2118, 193, 1403, 122, 192, 1358, 179, 122, 195, 17204, 181, 3361, 123, 11078, 2118, 176, 2087, 122, 2351, 1869, 122, 2853, 4272, 5182, 1704, 2704, 2638, 5184, 2663, 23220, 1179, 1979, 2704, 2657, 1278, 9468, 21440, 2118, 2755, 9468, 21440, 2118, 13075, 7629, 1477, 5144, 1161, 123, 2853, 2070, 6360, 1704, 2704, 2638, 5184, 2663, 23220, 1179, 1979, 2704, 2657, 1278, 9468, 21440, 2118, 2755, 9468, 21440, 2118, 13075, 7629, 1477, 5144, 1161, 7649, 8664, 9815, 6600, 2272, 18250, 3246, 188, 1605, 1558, 16798, 3621, 25575, 1958, 23639, 1162, 1664, 1353, 2765, 13093, 4182, 183, 1116, 1665, 1233, 1665, 4420, 1359, 1407, 2087, 23896, 2007, 10649, 1183, 1403, 7535, 13538, 1162, 185, 2155, 2875, 3484, 17744, 1106, 3702, 8944, 3254, 18505, 1106, 3702, 8944, 1407, 2087, 175, 1181, 1403, 11109, 172, 1204, 4069, 4420, 3507, 7542, 1193, 3659, 12645, 183, 1116, 1665, 1233, 1665, 1655, 1141, 1214, 2812, 18675, 1193, 4465, 9468, 21440, 2118, 23220, 1179, 1979, 2704, 179, 19762, 3113, 1857, 1260, 2093, 10615, 10351, 4420, 15819, 1407, 2087, 175, 1181, 1403, 11109, 172, 1204, 2229, 172, 1204, 2259, 2568, 4177, 4010, 3582, 6022, 189, 1830, 10841, 1775, 4718, 1821, 1183, 1403, 6919, 1161, 2533, 188, 1605, 6028, 12477, 8674, 2533, 6028, 12477, 8674, 3246, 171, 1918, 1884, 15789, 1616, 18593, 15355, 3884, 1884, 15789, 1616, 18593, 15355, 2794, 11019, 6063, 7140, 4420, 15965, 2452, 2915, 5884, 23639, 1162, 2259, 2568, 7300, 7898, 188, 1605, 171, 1918, 11019, 6063, 3402, 1160, 2114, 4420, 3233, 1822, 1344, 1821, 1183, 1403, 6919, 1161, 189, 1830, 10841, 1775, 2114, 2452, 3151, 1821, 1183, 1403, 6919, 1161, 189, 1830, 10841, 1775, 2452, 11019, 6063, 4420, 11019, 6063, 121, 5667, 1664, 11019, 1233, 6617, 11531, 1372, 11019, 6063, 121, 5667, 11019, 1233, 6617, 11531, 1372, 24181, 1643, 4371, 1143, 2852, 3622, 1215, 4928, 8115, 10642, 9366, 3997, 2774, 4071, 14133, 9455, 16016, 5408, 23639, 1162, 21014, 1884, 1775, 15122, 26302, 2235, 1982, 19774, 3187, 5320, 23639, 1162, 2686, 1703, 5706, 183, 1116, 1665, 1233, 1665, 4420, 1529, 4079, 5391, 128, 1429, 122, 1201, 1259, 5073, 3508, 5046, 121, 1492, 4420, 1492, 129, 23639, 1162, 1301, 4455, 5615, 4420, 5942, 123, 1664, 23639, 1162, 1372, 1821, 1183, 1403, 6919, 1161, 189, 1830, 10841, 1775, 1884, 15789, 1616, 18593, 15355, 3884, 11019, 6063, 23639, 1162, 1372, 20844, 2895, 1664, 23639, 1162, 1372, 2812, 1159, 182, 186, 1475, 186, 1495, 3746, 1627, 4650, 1808, 27574, 21014, 23639, 1162, 1344, 1821, 1183, 1403, 6919, 1161, 189, 1830, 10841, 1775, 1372, 2299, 1822, 1821, 1183, 1403, 6919, 1161, 189, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# ‚úÖ Charger le mod√®le pr√©-entra√Æn√©\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# üìä M√©triques d‚Äô√©valuation\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "    }\n",
        "\n",
        "# üîß Arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_biobert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# üöÄ Entra√Ænement avec Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "bfbUk07xYnsH",
        "outputId": "7bd0df67-4079-4fa6-e5a0-5a399130cad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-44-3145e7aac804>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='112' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [112/112 02:15, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.594200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.138300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=112, training_loss=0.3349013738334179, metrics={'train_runtime': 136.8084, 'train_samples_per_second': 6.549, 'train_steps_per_second': 0.819, 'total_flos': 234128244080640.0, 'train_loss': 0.3349013738334179, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âvaluer le mod√®le sur le jeu de validation\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Affichage des m√©triques principales\n",
        "print(\"üìä R√©sultats de l'√©valuation :\")\n",
        "print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "LL25oxRfYrIh",
        "outputId": "1b78e2a3-eb75-4e26-d3cc-78c7965b7fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats de l'√©valuation :\n",
            "Accuracy : 0.9549\n",
            "F1-score : 0.9549\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.99      0.98      0.98       162\n",
            "        GENE       0.89      0.97      0.93        64\n",
            "     PROTEIN       0.95      0.95      0.95       148\n",
            "     SYMPTOM       0.00      0.00      0.00         3\n",
            "\n",
            "   micro avg       0.95      0.95      0.95       377\n",
            "   macro avg       0.71      0.72      0.71       377\n",
            "weighted avg       0.95      0.95      0.95       377\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä √âvaluation sur le jeu de test\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# üìã Affichage des m√©triques\n",
        "print(\"üìä R√©sultats sur le jeu de test :\")\n",
        "print(f\"Accuracy : {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {test_results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(test_results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "2jDUXNcnYvaS",
        "outputId": "fba486ee-3706-4234-997d-316e30170f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats sur le jeu de test :\n",
            "Accuracy : 0.9563\n",
            "F1-score : 0.9563\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.98      1.00      0.99       144\n",
            "        GENE       0.90      0.94      0.92        78\n",
            "     PROTEIN       0.97      0.91      0.94        97\n",
            "     SYMPTOM       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       0.96      0.96      0.96       320\n",
            "   macro avg       0.96      0.96      0.96       320\n",
            "weighted avg       0.96      0.96      0.96       320\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = \"/content\"\n",
        "target_dir = \"/content/drive/MyDrive/NLP_dernier_modification_09_06_70_15_15\"\n",
        "\n",
        "# Cr√©er le dossier cible s‚Äôil n‚Äôexiste pas\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Parcourir tous les fichiers/dossiers dans /content sauf /content/drive\n",
        "for item in os.listdir(source_dir):\n",
        "    src_path = os.path.join(source_dir, item)\n",
        "    dst_path = os.path.join(target_dir, item)\n",
        "\n",
        "    if item == \"drive\":\n",
        "        continue  # ‚ö†Ô∏è Ignorer le dossier Google Drive\n",
        "\n",
        "    try:\n",
        "        if os.path.isdir(src_path):\n",
        "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur en copiant {item} : {e}\")\n",
        "\n",
        "print(f\"‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : {target_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUqkiWa8ZdPA",
        "outputId": "5b7b14e6-ee45-4934-f080-5b484eb846b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : /content/drive/MyDrive/NLP_dernier_modification_09_06_70_15_15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_______________________________________________________________"
      ],
      "metadata": {
        "id": "IZhjkyR_YY78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _60_20_20"
      ],
      "metadata": {
        "id": "h4Ui8VCoVvgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "\n",
        "# Download 'punkt' for word_tokenize (already done, but good practice to keep)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the specific 'punkt_tab' resource mentioned in the error\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# üìå Fonction principale pour annotation BIO\n",
        "def annotate_bio(df, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for _, row in df.iterrows():\n",
        "            abstract = row['cleaned']  # ‚úÖ colonne contenant le texte nettoy√©\n",
        "\n",
        "            # Assurez-vous que la colonne 'cleaned' n'est pas vide ou None\n",
        "            if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "                 continue # Ignore les lignes sans texte nettoy√©\n",
        "\n",
        "            try:\n",
        "                # Utiliser un ensemble vide si la colonne n'existe pas\n",
        "                # ou si l'√©valuation √©choue\n",
        "                genes = set(ast.literal_eval(row.get('genes', '[]')))\n",
        "                proteins = set(ast.literal_eval(row.get('proteins', '[]')))\n",
        "                diseases = set(ast.literal_eval(row.get('diseases', '[]')))\n",
        "                symptoms = set(ast.literal_eval(row.get('symptoms', '[]')))\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur parsing √† la ligne {row.name} : {e}\")\n",
        "                continue\n",
        "\n",
        "            # G√©rer les cas o√π abstract pourrait √™tre None ou non-string\n",
        "            if not isinstance(abstract, str):\n",
        "                 print(f\"Ligne {row.name} a un type d'abstract inattendu: {type(abstract)}\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            tokens = word_tokenize(abstract)\n",
        "\n",
        "            for token in tokens:\n",
        "                tag = \"O\"\n",
        "                token_lower = token.lower()\n",
        "                token_upper = token.upper()\n",
        "\n",
        "                # It√©rer sur les ensembles d'entit√©s\n",
        "                if token_upper in genes:\n",
        "                    tag = \"B-GENE\"\n",
        "                elif token_lower in proteins:\n",
        "                    tag = \"B-PROTEIN\"\n",
        "                elif token_lower in diseases:\n",
        "                    tag = \"B-DISEASE\"\n",
        "                elif token_lower in symptoms:\n",
        "                    tag = \"B-SYMPTOM\"\n",
        "\n",
        "                fout.write(f\"{token} {tag}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Fichier BIO sauvegard√© dans : {output_path}\")\n",
        "\n",
        "# üîÑ Chargement des fichiers\n",
        "train_df = pd.read_csv(\"/content/abstracts07_06_train_60_20_20.csv\")\n",
        "val_df   = pd.read_csv(\"/content/abstracts07_06_val_60_20_20.csv\")\n",
        "test_df  = pd.read_csv(\"/content/abstracts07_06_test_60_20_20.csv\")\n",
        "\n",
        "# üìù Application\n",
        "annotate_bio(train_df, \"/content/abstracts07_06_bio_train_60_20_20.bio\")\n",
        "annotate_bio(val_df, \"/content/abstracts07_06_bio_val_60_20_20.bio\")\n",
        "annotate_bio(test_df, \"/content/abstracts07_06_bio_test_60_20_20.bio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utpG__34behO",
        "outputId": "9fb3f2f9-f990-4850-b92e-37170e6b6687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_train_60_20_20.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_val_60_20_20.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_test_60_20_20.bio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Elle ignore les tokens \"O\" dans les labels pendant l'entra√Ænement.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# üìå Mod√®le utilis√© (BioBERT)\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# üè∑Ô∏è √âtiquettes\n",
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "\n",
        "# üìÑ Lire un fichier BIO et l'organiser par phrase\n",
        "def read_bio_file(filepath):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_tokens = []\n",
        "    current_labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == '':\n",
        "                if current_tokens:\n",
        "                    sentences.append(current_tokens)\n",
        "                    labels.append(current_labels)\n",
        "                    current_tokens = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                splits = line.strip().split()\n",
        "                if len(splits) == 2:\n",
        "                    token, tag = splits\n",
        "                    current_tokens.append(token)\n",
        "                    current_labels.append(tag)\n",
        "\n",
        "        # Ajouter la derni√®re phrase si non vide\n",
        "        if current_tokens:\n",
        "            sentences.append(current_tokens)\n",
        "            labels.append(current_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "# ‚úÇÔ∏è Tokenisation + alignement des labels avec gestion du max_length\n",
        "#    et exclusion des tokens \"O\" (ignor√©s avec -100)\n",
        "def tokenize_and_align(sentences, tags, tokenizer):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        sentences,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                tag = label[word_idx]\n",
        "                label_ids.append(label2id[tag] if tag != \"O\" else -100)\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# üìÅ Pr√©parer un Dataset HuggingFace depuis un fichier BIO\n",
        "def prepare_dataset_from_bio(filepath):\n",
        "    sents, tags = read_bio_file(filepath)\n",
        "    tokenized = tokenize_and_align(sents, tags, tokenizer)\n",
        "    return Dataset.from_dict(tokenized)\n",
        "\n",
        "\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "train_dataset = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_train_60_20_20.bio\")\n",
        "val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val_60_20_20.bio\")\n",
        "test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test_60_20_20.bio\")\n",
        "\n",
        "# üîç V√©rification\n",
        "print(\"‚úÖ Datasets cr√©√©s :\")\n",
        "print(\"Exemple train_dataset :\")\n",
        "print(train_dataset[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN5pSoUWbt0E",
        "outputId": "0b11ef95-a549-4334-f06b-5153a8bd9920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datasets cr√©√©s :\n",
            "Exemple train_dataset :\n",
            "{'input_ids': [101, 122, 188, 6617, 1231, 1643, 17881, 1571, 179, 3488, 127, 1405, 122, 1816, 1571, 9465, 1275, 9550, 1604, 188, 25892, 1571, 1580, 1604, 5507, 1571, 5347, 1580, 1571, 1545, 194, 8362, 17800, 1158, 3234, 9468, 2728, 1403, 1494, 1200, 9077, 1346, 10322, 9712, 1830, 26503, 23510, 1606, 12630, 4035, 25444, 2443, 1107, 16792, 185, 12937, 1161, 3262, 171, 1320, 13252, 2050, 175, 122, 176, 5709, 11071, 1320, 184, 123, 22245, 7291, 190, 124, 11580, 3740, 172, 125, 126, 22572, 10961, 16631, 172, 127, 1260, 5579, 1200, 194, 126, 1301, 19411, 1162, 173, 125, 3840, 27006, 176, 125, 2351, 1869, 122, 2587, 10093, 22572, 3484, 12809, 17288, 8362, 25105, 3150, 181, 13292, 1874, 1260, 9304, 26731, 12132, 23449, 1830, 8359, 1568, 9304, 13356, 5999, 1129, 1233, 5389, 1818, 175, 1643, 12937, 2225, 23449, 1830, 123, 8362, 11083, 181, 15136, 172, 1179, 1733, 4035, 1116, 1260, 181, 15136, 22233, 1200, 1306, 15276, 1197, 1571, 22737, 1580, 5682, 1306, 1665, 190, 11964, 10424, 5691, 22997, 1527, 181, 15136, 175, 10555, 124, 172, 1179, 1733, 1107, 3464, 178, 10294, 1233, 8362, 25105, 3150, 1260, 25338, 10582, 2042, 175, 22120, 7629, 9468, 7232, 175, 10555, 125, 2587, 10093, 22572, 3484, 12809, 17288, 8362, 25105, 3150, 181, 13292, 1874, 1260, 9304, 26731, 12132, 23449, 1830, 8359, 1568, 9304, 13356, 5999, 1129, 1233, 5389, 1818, 126, 1664, 7378, 2952, 8117, 2587, 8362, 25105, 3150, 181, 13292, 1874, 1260, 9304, 26731, 12132, 23449, 1830, 8359, 1568, 9304, 13356, 5999, 1129, 1233, 5389, 1818, 127, 172, 1179, 1733, 22233, 1200, 1306, 176, 4359, 8918, 8362, 25105, 3150, 172, 2879, 7578, 12686, 4121, 8376, 172, 2879, 7578, 175, 1200, 13141, 175, 10555, 3073, 4060, 1643, 9180, 1891, 12477, 27568, 1811, 9712, 1830, 26503, 188, 2430, 7147, 5668, 2765, 2765, 2838, 1119, 25710, 27054, 1785, 1723, 4344, 21293, 1880, 19687, 14911, 5047, 2765, 3367, 178, 1665, 1306, 3652, 174, 8508, 27184, 174, 8508, 2838, 9468, 2728, 1403, 2501, 15416, 5318, 174, 8508, 7012, 3238, 6664, 21439, 2227, 2838, 5320, 2320, 6183, 1936, 9468, 2728, 1403, 1494, 1200, 9077, 17689, 5565, 2838, 15661, 26468, 1421, 1159, 23245, 1423, 2765, 14715, 13590, 18882, 1665, 2233, 27948, 1606, 12630, 4035, 25444, 25220, 4929, 2765, 2765, 1119, 25710, 27054, 1785, 4035, 25444, 9468, 2728, 1403, 8609, 4709, 4283, 18107, 4844, 1479, 2765, 3324, 2765, 2016, 8080, 2501, 1648, 174, 8508, 14911, 12890, 27182, 4035, 25444, 21138, 1626, 9077, 1887, 1421, 2233, 27948, 3626, 1210, 9077, 185, 25534, 1306, 1475, 1177, 1775, 1477, 177, 1179, 2087, 1527, 1161, 2133, 15661, 26468, 2838, 6692, 15700, 9468, 2728, 1403, 1982, 5565, 12638, 2443, 1107, 16792, 1606, 3621, 16931, 1306, 9932, 23244, 18107, 14189, 15416, 1348, 24329, 2686, 3090, 1210, 9077, 8245, 1231, 6617, 1643, 2180, 7867, 14915, 9468, 2728, 1403, 2724, 2765, 2016, 12638, 17853, 21293, 1116, 7012, 12806, 6134, 1884, 2838, 2231, 10034, 3622, 1423, 2765, 14715, 13590, 18882, 1665, 2233, 8362, 17800, 1116, 2620, 1648, 185, 25534, 1306, 1475, 1177, 1775, 1477, 177, 1179, 2087, 1527, 1161, 2501, 9077, 21439, 21739, 1193, 4448, 9468, 2728, 1403, 19687, 178, 1665, 1306, 23510, 17881, 1571, 2351, 188, 9465, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# ‚úÖ Charger le mod√®le pr√©-entra√Æn√©\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# üìä M√©triques d‚Äô√©valuation\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "    }\n",
        "\n",
        "# üîß Arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_biobert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# üöÄ Entra√Ænement avec Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "extZMUtQbzEx",
        "outputId": "6c6d4181-bc32-495d-c266-e35766aa9bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-63-3145e7aac804>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 01:21, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.550300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=96, training_loss=0.3442710340023041, metrics={'train_runtime': 82.6827, 'train_samples_per_second': 9.289, 'train_steps_per_second': 1.161, 'total_flos': 200681352069120.0, 'train_loss': 0.3442710340023041, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âvaluer le mod√®le sur le jeu de validation\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Affichage des m√©triques principales\n",
        "print(\"üìä R√©sultats de l'√©valuation :\")\n",
        "print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Ncx9EdPmb_ky",
        "outputId": "a5f39e57-c844-4c9d-e197-b26f74ea4d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats de l'√©valuation :\n",
            "Accuracy : 0.9110\n",
            "F1-score : 0.9110\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.94      0.98      0.96       244\n",
            "        GENE       0.80      0.97      0.88       113\n",
            "     PROTEIN       0.95      0.82      0.88       210\n",
            "     SYMPTOM       0.00      0.00      0.00         6\n",
            "\n",
            "   micro avg       0.91      0.91      0.91       573\n",
            "   macro avg       0.67      0.69      0.68       573\n",
            "weighted avg       0.91      0.91      0.91       573\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä √âvaluation sur le jeu de test\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# üìã Affichage des m√©triques\n",
        "print(\"üìä R√©sultats sur le jeu de test :\")\n",
        "print(f\"Accuracy : {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {test_results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(test_results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "V2tdezcncIG3",
        "outputId": "d3b434f7-1dfc-4d04-c888-049f96cc4ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats sur le jeu de test :\n",
            "Accuracy : 0.9710\n",
            "F1-score : 0.9710\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.97      1.00      0.98       200\n",
            "        GENE       0.97      0.93      0.95        75\n",
            "     PROTEIN       0.98      0.94      0.96       104\n",
            "\n",
            "   micro avg       0.97      0.97      0.97       379\n",
            "   macro avg       0.97      0.96      0.97       379\n",
            "weighted avg       0.97      0.97      0.97       379\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = \"/content\"\n",
        "target_dir = \"/content/drive/MyDrive/NLP_dernier_modification_09_06_60_20_20\"\n",
        "\n",
        "# Cr√©er le dossier cible s‚Äôil n‚Äôexiste pas\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Parcourir tous les fichiers/dossiers dans /content sauf /content/drive\n",
        "for item in os.listdir(source_dir):\n",
        "    src_path = os.path.join(source_dir, item)\n",
        "    dst_path = os.path.join(target_dir, item)\n",
        "\n",
        "    if item == \"drive\":\n",
        "        continue  # ‚ö†Ô∏è Ignorer le dossier Google Drive\n",
        "\n",
        "    try:\n",
        "        if os.path.isdir(src_path):\n",
        "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur en copiant {item} : {e}\")\n",
        "\n",
        "print(f\"‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : {target_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9hfVRPhcIhL",
        "outputId": "bc246022-bde9-43b7-a15f-7afc09d67322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : /content/drive/MyDrive/NLP_dernier_modification_09_06_60_20_20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________________________________________________"
      ],
      "metadata": {
        "id": "kj0W8Poubex3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### _75_12.5_12.5"
      ],
      "metadata": {
        "id": "XgfCI1o4V19g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import ast\n",
        "\n",
        "# Download 'punkt' for word_tokenize (already done, but good practice to keep)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the specific 'punkt_tab' resource mentioned in the error\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# üìå Fonction principale pour annotation BIO\n",
        "def annotate_bio(df, output_path):\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "        for _, row in df.iterrows():\n",
        "            abstract = row['cleaned']  # ‚úÖ colonne contenant le texte nettoy√©\n",
        "\n",
        "            # Assurez-vous que la colonne 'cleaned' n'est pas vide ou None\n",
        "            if pd.isna(abstract) or abstract.strip() == \"\":\n",
        "                 continue # Ignore les lignes sans texte nettoy√©\n",
        "\n",
        "            try:\n",
        "                # Utiliser un ensemble vide si la colonne n'existe pas\n",
        "                # ou si l'√©valuation √©choue\n",
        "                genes = set(ast.literal_eval(row.get('genes', '[]')))\n",
        "                proteins = set(ast.literal_eval(row.get('proteins', '[]')))\n",
        "                diseases = set(ast.literal_eval(row.get('diseases', '[]')))\n",
        "                symptoms = set(ast.literal_eval(row.get('symptoms', '[]')))\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur parsing √† la ligne {row.name} : {e}\")\n",
        "                continue\n",
        "\n",
        "            # G√©rer les cas o√π abstract pourrait √™tre None ou non-string\n",
        "            if not isinstance(abstract, str):\n",
        "                 print(f\"Ligne {row.name} a un type d'abstract inattendu: {type(abstract)}\")\n",
        "                 continue\n",
        "\n",
        "\n",
        "            tokens = word_tokenize(abstract)\n",
        "\n",
        "            for token in tokens:\n",
        "                tag = \"O\"\n",
        "                token_lower = token.lower()\n",
        "                token_upper = token.upper()\n",
        "\n",
        "                # It√©rer sur les ensembles d'entit√©s\n",
        "                if token_upper in genes:\n",
        "                    tag = \"B-GENE\"\n",
        "                elif token_lower in proteins:\n",
        "                    tag = \"B-PROTEIN\"\n",
        "                elif token_lower in diseases:\n",
        "                    tag = \"B-DISEASE\"\n",
        "                elif token_lower in symptoms:\n",
        "                    tag = \"B-SYMPTOM\"\n",
        "\n",
        "                fout.write(f\"{token} {tag}\\n\")\n",
        "            fout.write(\"\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Fichier BIO sauvegard√© dans : {output_path}\")\n",
        "\n",
        "# üîÑ Chargement des fichiers\n",
        "train_df = pd.read_csv(\"/content/abstracts07_06_train_75_12.5_12.5.csv\")\n",
        "val_df   = pd.read_csv(\"/content/abstracts07_06_val_75_12.5_12.5.csv\")\n",
        "test_df  = pd.read_csv(\"/content/abstracts07_06_test_75_12.5_12.5.csv\")\n",
        "\n",
        "# üìù Application\n",
        "annotate_bio(train_df, \"/content/abstracts07_06_bio_train_75_12.5_12.5.bio\")\n",
        "annotate_bio(val_df, \"/content/abstracts07_06_bio_val_75_12.5_12.5.bio\")\n",
        "annotate_bio(test_df, \"/content/abstracts07_06_bio_test_75_12.5_12.5.bio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ohuMT5qeQU7",
        "outputId": "fc339a19-6137-4c70-f6c8-5887debbbd08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_train_75_12.5_12.5.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_val_75_12.5_12.5.bio\n",
            "‚úÖ Fichier BIO sauvegard√© dans : /content/abstracts07_06_bio_test_75_12.5_12.5.bio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Elle ignore les tokens \"O\" dans les labels pendant l'entra√Ænement.\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# üìå Mod√®le utilis√© (BioBERT)\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# üè∑Ô∏è √âtiquettes\n",
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "\n",
        "# üìÑ Lire un fichier BIO et l'organiser par phrase\n",
        "def read_bio_file(filepath):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_tokens = []\n",
        "    current_labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == '':\n",
        "                if current_tokens:\n",
        "                    sentences.append(current_tokens)\n",
        "                    labels.append(current_labels)\n",
        "                    current_tokens = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                splits = line.strip().split()\n",
        "                if len(splits) == 2:\n",
        "                    token, tag = splits\n",
        "                    current_tokens.append(token)\n",
        "                    current_labels.append(tag)\n",
        "\n",
        "        # Ajouter la derni√®re phrase si non vide\n",
        "        if current_tokens:\n",
        "            sentences.append(current_tokens)\n",
        "            labels.append(current_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "# ‚úÇÔ∏è Tokenisation + alignement des labels avec gestion du max_length\n",
        "#    et exclusion des tokens \"O\" (ignor√©s avec -100)\n",
        "def tokenize_and_align(sentences, tags, tokenizer):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        sentences,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                tag = label[word_idx]\n",
        "                label_ids.append(label2id[tag] if tag != \"O\" else -100)\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# üìÅ Pr√©parer un Dataset HuggingFace depuis un fichier BIO\n",
        "def prepare_dataset_from_bio(filepath):\n",
        "    sents, tags = read_bio_file(filepath)\n",
        "    tokenized = tokenize_and_align(sents, tags, tokenizer)\n",
        "    return Dataset.from_dict(tokenized)\n",
        "\n",
        "\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "train_dataset = prepare_dataset_from_bio(\"//content/abstracts07_06_bio_train_75_12.5_12.5.bio\")\n",
        "val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val_75_12.5_12.5.bio\")\n",
        "test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test_75_12.5_12.5.bio\")\n",
        "\n",
        "# üîç V√©rification\n",
        "print(\"‚úÖ Datasets cr√©√©s :\")\n",
        "print(\"Exemple train_dataset :\")\n",
        "print(train_dataset[0])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pj6lWuVeYGe",
        "outputId": "258d8752-a6af-4339-e6d2-baee811497a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datasets cr√©√©s :\n",
            "Exemple train_dataset :\n",
            "{'input_ids': [101, 122, 1619, 1920, 4182, 17881, 1571, 179, 3488, 130, 3081, 128, 3731, 1604, 9465, 1275, 1620, 1559, 188, 7629, 1571, 10973, 5507, 1571, 4925, 1545, 14541, 127, 1679, 2660, 3365, 5838, 6730, 2112, 13035, 14471, 4420, 15819, 1113, 2528, 13791, 1596, 6059, 12818, 3189, 27154, 3622, 178, 15197, 13292, 1766, 172, 122, 1195, 22654, 12210, 1200, 183, 1197, 123, 1126, 2571, 3309, 188, 1116, 123, 2351, 1869, 122, 2417, 190, 13166, 8032, 1596, 1113, 12241, 2853, 190, 13166, 15741, 2755, 16946, 1465, 22572, 1813, 7841, 3052, 2138, 191, 1161, 1366, 1161, 172, 1182, 1571, 1181, 190, 2497, 13836, 6066, 1324, 8916, 123, 2853, 15190, 18766, 4807, 2755, 16946, 1465, 22572, 1813, 7841, 3052, 2138, 191, 1161, 1366, 1161, 3007, 17459, 3209, 3772, 1679, 2660, 3365, 5838, 6730, 8115, 2112, 13035, 14471, 1113, 2528, 13791, 1596, 4420, 15819, 13467, 1231, 25461, 4069, 12818, 3189, 7091, 2200, 4013, 7356, 23972, 1158, 1679, 2660, 3365, 5838, 6730, 1113, 2528, 13791, 1596, 4420, 15819, 13467, 1231, 25461, 1502, 1539, 17881, 1527, 1982, 9712, 14017, 11030, 4611, 5127, 2598, 1884, 1732, 18194, 3340, 8703, 13950, 2199, 1529, 1231, 10182, 21629, 1714, 8115, 3653, 1714, 8115, 4182, 2747, 8115, 2905, 8115, 2112, 13035, 14471, 2686, 2908, 1545, 4237, 3626, 126, 1899, 10838, 9173, 4311, 14166, 1113, 2528, 13791, 1596, 4420, 4172, 26600, 183, 123, 1119, 4163, 2430, 15197, 18665, 183, 122, 3245, 8005, 1279, 4184, 19911, 1348, 183, 122, 2942, 10294, 6163, 12814, 3377, 183, 122, 14166, 4420, 14747, 7091, 2200, 1679, 2660, 3365, 5838, 6730, 9108, 1679, 2660, 3365, 5838, 6730, 8318, 3073, 13035, 1193, 183, 125, 16859, 9355, 123, 2277, 183, 122, 124, 1808, 183, 122, 16859, 1822, 183, 122, 1344, 183, 122, 7657, 3322, 6730, 6716, 2316, 1529, 3179, 183, 126, 12951, 183, 125, 179, 8032, 3375, 183, 123, 1719, 2041, 4612, 20121, 1348, 15604, 21155, 22354, 12562, 1619, 2527, 1679, 2660, 3365, 5838, 6730, 4725, 1107, 28092, 3621, 18472, 1279, 8508, 11412, 1183, 12691, 1679, 2660, 3365, 5838, 6730, 4725, 3653, 1714, 8115, 1141, 2025, 2112, 13035, 14471, 11271, 1679, 2660, 3365, 5838, 6730, 1654, 2114, 6593, 7815, 2527, 1329, 1679, 2660, 3365, 5838, 6730, 9927, 2952, 12691, 1437, 2418, 2629, 2112, 13035, 14471, 8115, 2425, 1322, 7587, 1141, 1529, 2527, 2076, 6730, 6716, 1179, 9177, 3409, 9505, 2609, 6876, 2060, 13426, 2960, 6029, 1785, 1679, 2660, 3365, 5838, 6730, 6716, 2316, 1444, 2174, 7356, 5605, 4959, 2629, 6730, 1263, 1858, 1113, 2528, 13791, 1596, 1322, 21521, 17881, 1571, 2351, 188, 9465, 1275, 1620, 1559, 188, 7629, 1571, 10973, 5507, 1571, 4925, 1545, 14541, 127, 9852, 2386, 27993, 1604, 19203, 1604, 1580, 7448, 1174, 1143, 28054, 2042, 4139, 2199, 4195, 11156, 1116, 4139, 2199, 5752, 14197, 6259, 4740, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [-100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# ‚úÖ Charger le mod√®le pr√©-entra√Æn√©\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# üìä M√©triques d‚Äô√©valuation\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "    }\n",
        "\n",
        "# üîß Arguments d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_biobert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# üöÄ Entra√Ænement avec Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "eYs0pkoHefOx",
        "outputId": "3c5d7c3d-c30c-4f39-f35e-95bdf6b428e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-56-3145e7aac804>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 01:39, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.556400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.122000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=120, training_loss=0.2962733010450999, metrics={'train_runtime': 100.2352, 'train_samples_per_second': 9.577, 'train_steps_per_second': 1.197, 'total_flos': 250851690086400.0, 'train_loss': 0.2962733010450999, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âvaluer le mod√®le sur le jeu de validation\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Affichage des m√©triques principales\n",
        "print(\"üìä R√©sultats de l'√©valuation :\")\n",
        "print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "09H7xgrOefzu",
        "outputId": "6790db1f-a249-4538-c3a0-c8aa7f400e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats de l'√©valuation :\n",
            "Accuracy : 0.9414\n",
            "F1-score : 0.9414\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       1.00      0.97      0.99       146\n",
            "        GENE       0.82      0.97      0.89        61\n",
            "     PROTEIN       0.95      0.90      0.92       116\n",
            "     SYMPTOM       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.94      0.94      0.94       324\n",
            "   macro avg       0.69      0.71      0.70       324\n",
            "weighted avg       0.94      0.94      0.94       324\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä √âvaluation sur le jeu de test\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# üìã Affichage des m√©triques\n",
        "print(\"üìä R√©sultats sur le jeu de test :\")\n",
        "print(f\"Accuracy : {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {test_results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(test_results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "go2oPHf2ejSJ",
        "outputId": "d58bc3e4-d51a-4233-b99e-a650fa758514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats sur le jeu de test :\n",
            "Accuracy : 0.9706\n",
            "F1-score : 0.9706\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.95      1.00      0.98       105\n",
            "        GENE       0.98      0.92      0.95        59\n",
            "     PROTEIN       0.98      0.97      0.98       107\n",
            "     SYMPTOM       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       0.97      0.97      0.97       272\n",
            "   macro avg       0.98      0.97      0.98       272\n",
            "weighted avg       0.97      0.97      0.97       272\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = \"/content\"\n",
        "target_dir = \"/content/drive/MyDrive/NLP_dernier_modification_09_06_75_12.5_12.5\"\n",
        "\n",
        "# Cr√©er le dossier cible s‚Äôil n‚Äôexiste pas\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Parcourir tous les fichiers/dossiers dans /content sauf /content/drive\n",
        "for item in os.listdir(source_dir):\n",
        "    src_path = os.path.join(source_dir, item)\n",
        "    dst_path = os.path.join(target_dir, item)\n",
        "\n",
        "    if item == \"drive\":\n",
        "        continue  # ‚ö†Ô∏è Ignorer le dossier Google Drive\n",
        "\n",
        "    try:\n",
        "        if os.path.isdir(src_path):\n",
        "            shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur en copiant {item} : {e}\")\n",
        "\n",
        "print(f\"‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : {target_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ntnd6XPemqA",
        "outputId": "acc20d25-25fc-4f44-bcb9-f42399637bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Tous les fichiers (sauf /drive) ont √©t√© copi√©s vers : /content/drive/MyDrive/NLP_dernier_modification_09_06_75_12.5_12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________"
      ],
      "metadata": {
        "id": "amAhZHLWeQpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test d'abilation Confeguration de Model"
      ],
      "metadata": {
        "id": "MkpdYR3Tki-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests d'Ablation pour NER BioBERT\n",
        "# Voici plusieurs variantes du mod√®le pour analyser l'impact de diff√©rents composants\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Configuration de base\n",
        "model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ TEST D'ABLATION 1: Mod√®le avec couches gel√©es (frozen layers)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def create_frozen_model(num_frozen_layers=6):\n",
        "    \"\"\"G√®le les N premi√®res couches de BioBERT\"\"\"\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(label_list),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # Geler les premi√®res couches\n",
        "    for i, layer in enumerate(model.bert.encoder.layer):\n",
        "        if i < num_frozen_layers:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    print(f\"‚úÖ Mod√®le cr√©√© avec {num_frozen_layers} couches gel√©es\")\n",
        "    return model\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ TEST D'ABLATION 2: Mod√®le avec dropout modifi√©\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def create_high_dropout_model(dropout_rate=0.3):\n",
        "    \"\"\"Augmente le dropout pour tester la r√©gularisation\"\"\"\n",
        "    model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=len(label_list),\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        hidden_dropout_prob=dropout_rate,\n",
        "        attention_probs_dropout_prob=dropout_rate\n",
        "    )\n",
        "    print(f\"‚úÖ Mod√®le cr√©√© avec dropout = {dropout_rate}\")\n",
        "    return model\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ TEST D'ABLATION 3: Mod√®le avec t√™te de classification simplifi√©e\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "class SimplifiedNERModel(nn.Module):\n",
        "    def __init__(self, base_model_name, num_labels):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModelForTokenClassification.from_pretrained(base_model_name).bert\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        # T√™te simplifi√©e : une seule couche lin√©aire au lieu de dropout + linear\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "def create_simplified_model():\n",
        "    \"\"\"Cr√©e un mod√®le avec une t√™te de classification simplifi√©e\"\"\"\n",
        "    model = SimplifiedNERModel(model_name, len(label_list))\n",
        "    print(\"‚úÖ Mod√®le cr√©√© avec t√™te de classification simplifi√©e\")\n",
        "    return model\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ TEST D'ABLATION 4: Mod√®le sans pr√©-entra√Ænement (poids al√©atoires)\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def create_random_weights_model():\n",
        "    \"\"\"Initialise le mod√®le avec des poids al√©atoires (pas de pr√©-entra√Ænement)\"\"\"\n",
        "    from transformers import BertConfig\n",
        "\n",
        "    config = BertConfig.from_pretrained(model_name)\n",
        "    config.num_labels = len(label_list)\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_config(config)\n",
        "    model.config.id2label = id2label\n",
        "    model.config.label2id = label2id\n",
        "\n",
        "    print(\"‚úÖ Mod√®le cr√©√© avec poids al√©atoires (sans pr√©-entra√Ænement)\")\n",
        "    return model\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ TEST D'ABLATION 5: Mod√®le avec moins de couches d'attention\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def create_fewer_layers_model(num_layers=6):\n",
        "    \"\"\"Cr√©e un mod√®le avec moins de couches transformer\"\"\"\n",
        "    from transformers import BertConfig\n",
        "\n",
        "    config = BertConfig.from_pretrained(model_name)\n",
        "    config.num_hidden_layers = num_layers  # R√©duire de 12 √† 6 couches\n",
        "    config.num_labels = len(label_list)\n",
        "\n",
        "    model = AutoModelForTokenClassification.from_config(config)\n",
        "    model.config.id2label = id2label\n",
        "    model.config.label2id = label2id\n",
        "\n",
        "    print(f\"‚úÖ Mod√®le cr√©√© avec {num_layers} couches au lieu de 12\")\n",
        "    return model\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ FONCTION D'ENTRA√éNEMENT G√âN√âRALIS√âE\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def train_ablation_model(model, train_dataset, val_dataset, test_dataset,\n",
        "                        experiment_name, epochs=4, lr=2e-5):\n",
        "    \"\"\"Entra√Æne et √©value un mod√®le d'ablation\"\"\"\n",
        "\n",
        "    # M√©triques d'√©valuation\n",
        "    def compute_metrics(p):\n",
        "        preds = np.argmax(p.predictions, axis=2)\n",
        "        labels = p.label_ids\n",
        "\n",
        "        true_predictions = [\n",
        "            [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "            for pred, label in zip(preds, labels)\n",
        "        ]\n",
        "        true_labels = [\n",
        "            [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "            for pred, label in zip(preds, labels)\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "            \"f1\": f1_score(true_labels, true_predictions),\n",
        "            \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "        }\n",
        "\n",
        "    # Arguments d'entra√Ænement\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./ner_biobert_{experiment_name}\",\n",
        "        do_train=True,\n",
        "        do_eval=True,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        learning_rate=lr,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f\"./logs_{experiment_name}\",\n",
        "        logging_steps=50,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    # Entra√Ænement\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüöÄ D√©but de l'entra√Ænement : {experiment_name}\")\n",
        "    trainer.train()\n",
        "\n",
        "    # √âvaluation\n",
        "    val_results = trainer.evaluate()\n",
        "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "    print(f\"\\nüìä R√©sultats {experiment_name}:\")\n",
        "    print(f\"Validation - Accuracy: {val_results['eval_accuracy']:.4f}, F1: {val_results['eval_f1']:.4f}\")\n",
        "    print(f\"Test - Accuracy: {test_results['eval_accuracy']:.4f}, F1: {test_results['eval_f1']:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'experiment': experiment_name,\n",
        "        'val_accuracy': val_results['eval_accuracy'],\n",
        "        'val_f1': val_results['eval_f1'],\n",
        "        'test_accuracy': test_results['eval_accuracy'],\n",
        "        'test_f1': test_results['eval_f1']\n",
        "    }\n",
        "\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üß™ EXEMPLE D'UTILISATION - TESTS D'ABLATION\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "def run_ablation_study(train_dataset, val_dataset, test_dataset):\n",
        "    \"\"\"Lance tous les tests d'ablation\"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Test 1: Mod√®le de base (r√©f√©rence)\n",
        "    print(\"=\"*60)\n",
        "    print(\"üß™ TEST 1: Mod√®le de base (BioBERT complet)\")\n",
        "    print(\"=\"*60)\n",
        "    base_model = AutoModelForTokenClassification.from_pretrained(\n",
        "        model_name, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
        "    )\n",
        "    results.append(train_ablation_model(\n",
        "        base_model, train_dataset, val_dataset, test_dataset, \"baseline\"\n",
        "    ))\n",
        "\n",
        "    # Test 2: Couches gel√©es\n",
        "    print(\"=\"*60)\n",
        "    print(\"üß™ TEST 2: Mod√®le avec 6 couches gel√©es\")\n",
        "    print(\"=\"*60)\n",
        "    frozen_model = create_frozen_model(num_frozen_layers=6)\n",
        "    results.append(train_ablation_model(\n",
        "        frozen_model, train_dataset, val_dataset, test_dataset, \"frozen_6_layers\"\n",
        "    ))\n",
        "\n",
        "    # Test 3: Dropout √©lev√©\n",
        "    print(\"=\"*60)\n",
        "    print(\"üß™ TEST 3: Mod√®le avec dropout √©lev√© (0.3)\")\n",
        "    print(\"=\"*60)\n",
        "    high_dropout_model = create_high_dropout_model(dropout_rate=0.3)\n",
        "    results.append(train_ablation_model(\n",
        "        high_dropout_model, train_dataset, val_dataset, test_dataset, \"high_dropout\"\n",
        "    ))\n",
        "\n",
        "    # Test 4: T√™te simplifi√©e\n",
        "    print(\"=\"*60)\n",
        "    print(\"üß™ TEST 4: Mod√®le avec t√™te de classification simplifi√©e\")\n",
        "    print(\"=\"*60)\n",
        "    simplified_model = create_simplified_model()\n",
        "    results.append(train_ablation_model(\n",
        "        simplified_model, train_dataset, val_dataset, test_dataset, \"simplified_head\"\n",
        "    ))\n",
        "\n",
        "    # Test 5: Poids al√©atoires\n",
        "    print(\"=\"*60)\n",
        "    print(\"üß™ TEST 5: Mod√®le sans pr√©-entra√Ænement (poids al√©atoires)\")\n",
        "    print(\"=\"*60)\n",
        "    random_model = create_random_weights_model()\n",
        "    results.append(train_ablation_model(\n",
        "        random_model, train_dataset, val_dataset, test_dataset, \"random_weights\", epochs=5, lr=5e-5\n",
        "    ))\n",
        "\n",
        "    # R√©sum√© des r√©sultats\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üìä R√âSUM√â DES TESTS D'ABLATION\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"{'Exp√©rience':<20} {'Val Acc':<10} {'Val F1':<10} {'Test Acc':<10} {'Test F1':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for result in results:\n",
        "        print(f\"{result['experiment']:<20} {result['val_accuracy']:<10.4f} {result['val_f1']:<10.4f} \"\n",
        "              f\"{result['test_accuracy']:<10.4f} {result['test_f1']:<10.4f}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "S5m_HO94krWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "# üöÄ LANCEMENT DES TESTS\n",
        "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "\n",
        "# Supposons que vous avez d√©j√† vos datasets charg√©s\n",
        "# train_dataset = prepare_dataset_from_bio(\"path/to/train.bio\")\n",
        "# val_dataset = prepare_dataset_from_bio(\"path/to/val.bio\")\n",
        "# test_dataset = prepare_dataset_from_bio(\"path/to/test.bio\")\n",
        "\n",
        "train_dataset = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_train_60_20_20.bio\")\n",
        "val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val_60_20_20.bio\")\n",
        "test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test_60_20_20.bio\")\n",
        "\n",
        "# Lancer l'√©tude d'ablation compl√®te\n",
        "ablation_results = run_ablation_study(train_dataset, val_dataset, test_dataset)\n",
        "\n",
        "# OU lancer un test individuel, par exemple :\n",
        "# model = create_frozen_model(num_frozen_layers=8)\n",
        "# result = train_ablation_model(model, train_dataset, val_dataset, test_dataset, \"frozen_8_layers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JosZfSGrkwOM",
        "outputId": "96b012ff-ac66-4573-e8db-d606160aa63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üß™ TEST 1: Mod√®le de base (BioBERT complet)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-73-991c9516662b>:172: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ D√©but de l'entra√Ænement : baseline\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 02:04, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.551100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä R√©sultats baseline:\n",
            "Validation - Accuracy: 0.9092, F1: 0.9092\n",
            "Test - Accuracy: 0.9789, F1: 0.9789\n",
            "============================================================\n",
            "üß™ TEST 2: Mod√®le avec 6 couches gel√©es\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-73-991c9516662b>:172: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mod√®le cr√©√© avec 6 couches gel√©es\n",
            "\n",
            "üöÄ D√©but de l'entra√Ænement : frozen_6_layers\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 01:50, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.700100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä R√©sultats frozen_6_layers:\n",
            "Validation - Accuracy: 0.8866, F1: 0.8866\n",
            "Test - Accuracy: 0.9604, F1: 0.9604\n",
            "============================================================\n",
            "üß™ TEST 3: Mod√®le avec dropout √©lev√© (0.3)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-73-991c9516662b>:172: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mod√®le cr√©√© avec dropout = 0.3\n",
            "\n",
            "üöÄ D√©but de l'entra√Ænement : high_dropout\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 01:49, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.832700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä R√©sultats high_dropout:\n",
            "Validation - Accuracy: 0.8761, F1: 0.8761\n",
            "Test - Accuracy: 0.9578, F1: 0.9578\n",
            "============================================================\n",
            "üß™ TEST 4: Mod√®le avec t√™te de classification simplifi√©e\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-73-991c9516662b>:172: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Mod√®le cr√©√© avec t√™te de classification simplifi√©e\n",
            "\n",
            "üöÄ D√©but de l'entra√Ænement : simplified_head\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 01:49, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.608500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä R√©sultats simplified_head:\n",
            "Validation - Accuracy: 0.8743, F1: 0.8743\n",
            "Test - Accuracy: 0.9551, F1: 0.9551\n",
            "============================================================\n",
            "üß™ TEST 5: Mod√®le sans pr√©-entra√Ænement (poids al√©atoires)\n",
            "============================================================\n",
            "‚úÖ Mod√®le cr√©√© avec poids al√©atoires (sans pr√©-entra√Ænement)\n",
            "\n",
            "üöÄ D√©but de l'entra√Ænement : random_weights\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-991c9516662b>:172: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [120/120 02:04, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.187500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:03]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä R√©sultats random_weights:\n",
            "Validation - Accuracy: 0.8080, F1: 0.8080\n",
            "Test - Accuracy: 0.9129, F1: 0.9129\n",
            "\n",
            "================================================================================\n",
            "üìä R√âSUM√â DES TESTS D'ABLATION\n",
            "================================================================================\n",
            "Exp√©rience           Val Acc    Val F1     Test Acc   Test F1   \n",
            "--------------------------------------------------------------------------------\n",
            "baseline             0.9092     0.9092     0.9789     0.9789    \n",
            "frozen_6_layers      0.8866     0.8866     0.9604     0.9604    \n",
            "high_dropout         0.8761     0.8761     0.9578     0.9578    \n",
            "simplified_head      0.8743     0.8743     0.9551     0.9551    \n",
            "random_weights       0.8080     0.8080     0.9129     0.9129    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2eme modele"
      ],
      "metadata": {
        "id": "-uaKWuUFcBQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOTxJ3wid6Rx",
        "outputId": "cae41626-ffc7-43f5-b1cb-21a104f1f90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N9QdMxbRfcXH",
        "outputId": "d3c845d6-befa-4ef2-fd3b-8bc738397fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "datasets"
                ]
              },
              "id": "d4aec095e11947c0845dbe4cf248d6bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}"
      ],
      "metadata": {
        "id": "RqJ5-3ZOfwti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz6QnB0wgkk9",
        "outputId": "9cc3784c-3102-4061-f146-fc93fd496a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = ['O', 'B-GENE', 'B-PROTEIN', 'B-DISEASE', 'B-SYMPTOM']\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "\n",
        "# üìÑ Lire un fichier BIO et l'organiser par phrase\n",
        "def read_bio_file(filepath):\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_tokens = []\n",
        "    current_labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == '':\n",
        "                if current_tokens:\n",
        "                    sentences.append(current_tokens)\n",
        "                    labels.append(current_labels)\n",
        "                    current_tokens = []\n",
        "                    current_labels = []\n",
        "            else:\n",
        "                splits = line.strip().split()\n",
        "                if len(splits) == 2:\n",
        "                    token, tag = splits\n",
        "                    current_tokens.append(token)\n",
        "                    current_labels.append(tag)\n",
        "\n",
        "        # Ajouter la derni√®re phrase si non vide\n",
        "        if current_tokens:\n",
        "            sentences.append(current_tokens)\n",
        "            labels.append(current_labels)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "# ‚úÇ Tokenisation + alignement des labels avec gestion du max_length\n",
        "#    et exclusion des tokens \"O\" (ignor√©s avec -100)\n",
        "def tokenize_and_align(sentences, tags, tokenizer):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        sentences,\n",
        "        is_split_into_words=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None\n",
        "    )\n",
        "\n",
        "    aligned_labels = []\n",
        "    for i, label in enumerate(tags):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                tag = label[word_idx]\n",
        "                label_ids.append(label2id[tag] if tag != \"O\" else -100)\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        aligned_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = aligned_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "# üìÅ Pr√©parer un Dataset HuggingFace depuis un fichier BIO\n",
        "def prepare_dataset_from_bio(filepath):\n",
        "    sents, tags = read_bio_file(filepath)\n",
        "    tokenized = tokenize_and_align(sents, tags, tokenizer)\n",
        "    return Dataset.from_dict(tokenized)"
      ],
      "metadata": {
        "id": "slp4XTmwhYR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_dataset = prepare_dataset_from_bio(\"/content/drive/MyDrive/NLP_dernier_modification_07_06/abstracts07_06_bio_train.bio\")\n",
        "#val_dataset   = prepare_dataset_from_bio(\"/content/drive/MyDrive/NLP_dernier_modification_07_06/abstracts07_06_bio_val.bio\")\n",
        "#test_dataset  = prepare_dataset_from_bio(\"/content/drive/MyDrive/NLP_dernier_modification_07_06/abstracts07_06_bio_test.bio\")\n",
        "\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "# train_dataset = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_train_60_20_20.bio\")\n",
        "# val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val_60_20_20.bio\")\n",
        "# test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test_60_20_20.bio\")\n",
        "\n",
        "# ‚úÖ Cr√©er les datasets avec les bons fichiers\n",
        "train_dataset = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_train.bio\")\n",
        "val_dataset   = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_val.bio\")\n",
        "test_dataset  = prepare_dataset_from_bio(\"/content/abstracts07_06_bio_test.bio\")"
      ],
      "metadata": {
        "id": "eumOSVH4geK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________________________________________"
      ],
      "metadata": {
        "id": "KqkP3hJRvfm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# üìå Nom du mod√®le\n",
        "model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
        "\n",
        "# Charger le tokenizer et le mod√®le\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# M√©triques NER\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "    }\n",
        "\n",
        "# ‚úÖ Configuration d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_pubmedbert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# üöÄ Entra√Ænement\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "e4UB4Mfulb54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "1aaf14cd-6c8f-4a95-8781-5d099cc2e848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-28-fc4f0a1e73ae>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 01:55, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.575800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.150900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=128, training_loss=0.2989308377727866, metrics={'train_runtime': 116.8177, 'train_samples_per_second': 8.766, 'train_steps_per_second': 1.096, 'total_flos': 267575136092160.0, 'train_loss': 0.2989308377727866, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_______________________________________________________________________"
      ],
      "metadata": {
        "id": "6CX3ALzLwcgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UfMqEmrcwwk",
        "outputId": "98a01885-5539-4bec-bb5d-f443d5f287e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.52.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âvaluer le mod√®le sur le jeu de validation\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Affichage des m√©triques principales\n",
        "print(\"üìä R√©sultats de l'√©valuation :\")\n",
        "print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "-QJUBX86gOiK",
        "outputId": "967db925-2b6e-4d9c-b48b-9488b14c8835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats de l'√©valuation :\n",
            "Accuracy : 0.9267\n",
            "F1-score : 0.9267\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.99      0.96      0.98       103\n",
            "        GENE       0.90      0.86      0.88        63\n",
            "     PROTEIN       0.88      0.93      0.91       105\n",
            "     SYMPTOM       1.00      1.00      1.00         2\n",
            "\n",
            "   micro avg       0.93      0.93      0.93       273\n",
            "   macro avg       0.94      0.94      0.94       273\n",
            "weighted avg       0.93      0.93      0.93       273\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä √âvaluation sur le jeu de test\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# üìã Affichage des m√©triques\n",
        "print(\"üìä R√©sultats sur le jeu de test :\")\n",
        "print(f\"Accuracy : {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {test_results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(test_results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "9XkXFC1fiEhW",
        "outputId": "abb3dc1d-3280-4443-c4ed-de54117ddea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats sur le jeu de test :\n",
            "Accuracy : 0.9585\n",
            "F1-score : 0.9585\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.99      0.99      0.99       120\n",
            "        GENE       0.93      0.95      0.94        66\n",
            "     PROTEIN       0.94      0.92      0.93        78\n",
            "     SYMPTOM       0.00      0.00      0.00         1\n",
            "\n",
            "   micro avg       0.96      0.96      0.96       265\n",
            "   macro avg       0.71      0.72      0.72       265\n",
            "weighted avg       0.96      0.96      0.96       265\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________________________________________________"
      ],
      "metadata": {
        "id": "GF_p1july2dI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3 eme modele**"
      ],
      "metadata": {
        "id": "vq83bxb4jNEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "# üìå Nom du mod√®le SapBERT\n",
        "model_name = \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\"\n",
        "\n",
        "# Charger le tokenizer et le mod√®le\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# üîç M√©triques de NER\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=2)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    true_predictions = [\n",
        "        [id2label[p] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [id2label[l] for (p, l) in zip(pred, label) if l != -100]\n",
        "        for pred, label in zip(preds, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions, output_dict=False)\n",
        "    }\n",
        "\n",
        "# ‚úÖ Configuration d'entra√Ænement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_sapbert\",\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "# ‚öôÔ∏è Trainer HuggingFace\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# üöÄ Lancer l'entra√Ænement\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "9hFFkycejS5I",
        "outputId": "1964430f-914c-4be4-df2b-a472b2f44c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cambridgeltl/SapBERT-from-PubMedBERT-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-22-bcb7a37bf3d4>:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [128/128 01:46, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.133200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.696900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=128, training_loss=0.8190451115369797, metrics={'train_runtime': 106.9887, 'train_samples_per_second': 9.571, 'train_steps_per_second': 1.196, 'total_flos': 267575136092160.0, 'train_loss': 0.8190451115369797, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# √âvaluer le mod√®le sur le jeu de validation\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Affichage des m√©triques principales\n",
        "print(\"üìä R√©sultats de l'√©valuation :\")\n",
        "print(f\"Accuracy : {results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "ESOJbJMVj3e3",
        "outputId": "083c28b3-4c18-4337-fa86-67789b92b4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats de l'√©valuation :\n",
            "Accuracy : 0.7889\n",
            "F1-score : 0.7889\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.89      0.91      0.90       127\n",
            "        GENE       0.86      0.48      0.62        67\n",
            "     PROTEIN       0.63      0.92      0.75        71\n",
            "     SYMPTOM       0.00      0.00      0.00         5\n",
            "\n",
            "   micro avg       0.79      0.79      0.79       270\n",
            "   macro avg       0.60      0.58      0.57       270\n",
            "weighted avg       0.80      0.79      0.77       270\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä √âvaluation sur le jeu de test\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# üìã Affichage des m√©triques\n",
        "print(\"üìä R√©sultats sur le jeu de test :\")\n",
        "print(f\"Accuracy : {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"F1-score : {test_results['eval_f1']:.4f}\")\n",
        "print(\"\\nüìÑ Rapport d√©taill√© :\")\n",
        "print(test_results['eval_report'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "dZ_-KIk0lU9W",
        "outputId": "a3ecc165-6a3a-43d8-c88d-ed87eb76227f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 01:16]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä R√©sultats sur le jeu de test :\n",
            "Accuracy : 0.7438\n",
            "F1-score : 0.7438\n",
            "\n",
            "üìÑ Rapport d√©taill√© :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     DISEASE       0.82      0.96      0.89       121\n",
            "        GENE       0.58      0.32      0.41        60\n",
            "     PROTEIN       0.68      0.82      0.74        87\n",
            "     SYMPTOM       1.00      0.23      0.38        13\n",
            "\n",
            "   micro avg       0.74      0.74      0.74       281\n",
            "   macro avg       0.77      0.58      0.60       281\n",
            "weighted avg       0.73      0.74      0.72       281\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "geykiFirmqUA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}